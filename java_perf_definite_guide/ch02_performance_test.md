# Ch2 성능 테스트 접근법

## 세가지 종류
1. 마이크로 벤치마크
 - 함수나 특정 로직 단위
2. 매크로 벤치마크
 - 실제와 동일한 전체 애플리케이션
3. 메조 벤치마크
 - 마이크로와 매크로의 중간


## 가장 큰 테스트 원칙

> **실제 애플리케이션을 테스트 하자**
- 제품이 실제 사용되는 방식 그대로


### 마이크로 벤치마크

- 예)
 - 동기화 메서드 대비 비동기화 메서드 선능 비교
 - 스레드 1개 v.s 스레드풀 비교

> **반드시 직접적인 결과를 사용해야 한다.**
- 결과를 사용하지 않으면 컴파일러가 최적화시켜서 코드 자체가 증발될수도
- volatile로 선언하면 된다.


> **관련없는 동작을 포함해서는 안된다.**
- 예를 들어 random으로 입력 변수를 정의하는 경우
- 이 부분은 계산시간에 포함시키면 안된다.
- 안그려먼 random 함수의 성능 측정이 된다.

> 정확한 입력 값을 측정해야 한다.
- 위험 요소는 테스트의 입력 값 범위이다
- 미리 계산해서 준비하거나 미리 validation을 하는 것이 좋다.

> **준비 기간을 거치는 것이 좋다.**
- 일종의 warmup, 컴파일러가 최적 코드 생성할 기회
- 간단히 측정하고자 하는 로직을 2번 부르고 2번째만 시간 측정

마이크로 벤치마크를 잘 찌는 건 어렵다.

### 매크로 벤치마크

- What
 - 외부 자원을 포함한 실제 애플리케이션 자체 성능 측정
- HowTo
 - 모든 환경 설정을 동일하게(느린 DB, LDAP을 통한 권한 체크 있다면 그대로)
- Why
 - 부분 요소들의 성능의 합이 시스템의 성능이 되지 않는다.
 - 특정 요소의 향상보다는 병목 구간을 찾아서 개선해야 한다.

> ***여러 개의 JVM으로 전체 시스템 테스트하자***
- 다수 app을 동시에 하나의 HW에서 동작시 문제점 파악
- Why? JVM의 디폴트 설정은 자신이 혼자 HW를 독점할때를 가정한 것 =

### 메조 벤치마크

- What
 - 실제 app에서 성능 부분을 제외한 일부 비본질적인 요소를 생략/간략화한 것
   - ex) 권한 체크, 세션관리, 보안
- Why
 - 자동 테스트, 특히 모듈 단계에서 활용하기 좋다.


## 세가지 측정

### 대량의 배치(Batch)로 처리시간 측정

- Why
 - 너무 짧고, 제한된 범위로 시간 측정시 데이터 cache 효과 배제 어려움
   - ex) DB에서 가져오는 것이 아니라 JDBC의 local cache hit
 - 너무 짧으면 준비기간 내가 되므로 JIT 최적화가 이루어지기 전이다.
- HowTo
 - 광범위한 부분으로 처리, or for loop로 대량 처리

### 처리율 측정

- What
 - 일정 시간 내에 서버에서 완료할 수 있는 업무량
- HowTo
 - 클라이언트에서 사고 시간(think time)이 없어야 한다.
 - 너무 느린 클라이언트 사용은 AntiPattern,
   - 이는 서버의 처리율이 아니라 클라이언트의 처리율을 측정하는 셈

### 응답시간 테스트

- What
 - 클라이언트에서 요청을 보낸 후 응답을 받기까지 걸리는 응답 시간
- HowTo
 - 매우 간간히 보내야 한다
 - ex) 하나보내고 응답 받고, 30초 쉬었다가 다시
 - 서버의 부하량(총 처리율)은 일정하게 유지해야

> **응답 시간의 측정은 평균보다는 90%, 10% 로 기술**
> - ex) 응답의 90% 가 1.5초보다 빨랐고 10%는 1.5보다 느렸다.
> - 평균으로 하면 outlier가 있을 때 왜곡

## 변동성 이해하기

> **여러 번 반복해서 성능 측정을 하고 이를 평균한다**
> - 테스트 할 때마다 다른 성능 결과가 나올 수 있다.

> **회귀 테스트** 에 대해서는
- Null 가설은 base코드와 신규 코드상의 성능 차이가 없다.
- 목표가 되는 신뢰수준을 정한 후, ex 95%
- 이를 만족하는 시행 횟수만큼 수행하고
- baseline과 신규 코드의 평균 성능을 각각 잰다.
- ex) 95%의 신뢰수준으로 신규 코드는 25% 성능 향상이 있다.

## 성능 매니아들의 마지막 권고

> 아주 이상적으로는 **개발 주기 중 성능 테스트** 를 필수로 하자
- 커밋 전에 마이크로스픽하게 선응 테스트
- 이도 일정 신뢰수준으로 하려면 다량의 반복 테스트 필수

> **성능 테스트 전체를 자동화**
- 반복 테스트, t-분석까지 자동화

> **수집 가능한 모든 요소를 모으자**
- App log, System log, GC log
- Java Flight Record
- 스래드 스택, 막대 그래프, 힙 분석 데이터, 프로파일링, 힙 덤프
- 주변 요소들 정보. ex) DB의 성능 보고서, DB 시스템 통계 보고서

> **동일한 HW 환경에서 실행**
- 돈이 많이 들어 더 간단한 HW에서 테스트하면 성능 근사치를 얻는 것
- 실제랑 완전히 다를 수 있다.









---
