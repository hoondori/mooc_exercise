{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reasoning about agents\n",
    "\n",
    "\n",
    "## Inverse Reinforcement Learning\n",
    "---\n",
    "지금까지는 주어진 환경(MDP,POMDP), 주어진 utility function에서 최적의 action을 도출하는 것이 목적\n",
    "\n",
    "반면에, Agent의 행위(action)으로부터 agent 자체를 배우려는 것이 목적인 상황도 있다.\n",
    "\n",
    "* Inverse Reinforcement learning\n",
    "* learn human’s preference given observation, ex) 추천시스템 \n",
    "* c.f) generative models of human action\n",
    "\n",
    "![](./images/irl/01.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning about an agent from their actions: motivating example\n",
    "---\n",
    "\n",
    "### Donut trajectory\n",
    "\n",
    "* 도넛을 좋아한 것일수도 있고, 시간이 없으니 제일 가까운 식당에 간 것일 수도 있다. \n",
    "* 코드 및 결과 확인\n",
    "\n",
    "### 음식 선호 조사 \n",
    "\n",
    "* 각 음식 선호에 bias가 된 utility function들을 prior로 설계 후\n",
    "* donut trajectory를 observation data로 한 선호(favorite) posterior 조사\n",
    "* posterior에서는 donut 선호 경향이 나타남\n",
    "* 코드 및 결과 확인\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning about an agent from their actions: formalization\n",
    "---\n",
    "* 주어진 state-action pair sequence에서 Utility에 대한 posterior 추론\n",
    "\n",
    "![](./images/irl/02.png)\n",
    "![](./images/irl/03.png)\n",
    "![](./images/irl/04.png)\n",
    "\n",
    "* if there are gaps in the sequence or if we observe only the agent’s states (not the actions), then we need to marginalize over actions that were unobserved\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Inference from part of a sequence of actions\n",
    "---\n",
    "### partial trajectory로부터 의도 추론\n",
    "* [3,1] to [2,1] by moving left\n",
    "* 한 칸 움직이는 것일뿐이지만 South 도넛 선호 강한 암시 \n",
    "\n",
    "### 최대 선호인 donut 말고 누들과 veg 에 대한 선호는?\n",
    "* 누들과 Veg는 거의 동일한 prior 유지 (아무런 증거가 없었으므로)\n",
    "* Random start,long-sequence 관찰을 하더라도 low action noise 상황에서는 이들의 선호 강도가 안드러남\n",
    "\n",
    "###  Unidentifiability 문제\n",
    "*  Amin and Singh (2016)\n",
    " * low action noise => exploration이 거의 없어서 \n",
    " * 같은 관측으로도 여러 가능성을 설명할 수 있는 모호성이 있어서 \n",
    "    * ex) high action noise vs false-belief\n",
    " * active learning 으로 접근 \n",
    "  \n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Inferring The Cost of Time and Softmax Noise\n",
    "---\n",
    "timeCost와 action noise에 대한 추론\n",
    "\n",
    "결과 해석 \n",
    "* 입력: donutSouthTrajectory\n",
    "* 도넛 선호가 크지 않았는데도 left로 이동한 이유\n",
    "  * 상대적으로 action noise 작다(즉 이유없이 left로 가질 않을 듯)\n",
    "  * 상대적으로 high time cost( 음식 선호보다는 시간 절약 선호로 간듯 )\n",
    "\n",
    "좀더 긴 sequence에 대해 실험\n",
    "* donutN으로 가는 것과 donutS로 가는 것의 concat 이 입력 \n",
    "* 결과 : 도넛 선호가 상대적으로 강해졌다. action noise는 moderate, timeCost는 negligable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning about agents in POMDPs: Formalization\n",
    "---\n",
    "![](./images/irl/05.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application: Bandits\n",
    "---\n",
    "### RL in Bandits\n",
    "* unknown mapping from arms to non-numeric prizes (초콜릿,샴페인,꽝)\n",
    "* 음식에 대한 agent 선호가 알려짐\n",
    "* agent가 여러 시도를 해보면서 unknown mapping을 밝혀냄\n",
    "\n",
    "### IRL in Bandits\n",
    "* known the mapping from arms to prizes\n",
    "* 주어진 agent의 action들로부터 거꾸러 agent의 음식 선호를 추정(learning agent)\n",
    "\n",
    "### 첫번째 예제: Simple agent\n",
    "* agent prior belief known \n",
    "\n",
    "![](./images/irl/06.png)\n",
    "\n",
    "\n",
    "* arm1 을 5번 모두 당김\n",
    "* 음식 선호 경향(Utility) 추론\n",
    "  * 강한 샴페인 선호 추론\n",
    "  \n",
    "  \n",
    "### 두번째 예제:  prior belief unknown\n",
    "\n",
    "![](./images/irl/07.png)\n",
    "\n",
    "* 5번 정도의 trial로는 선호 모호\n",
    "* trial이 증가할수록 선호가 더 선명해짐\n",
    "\n",
    "\n",
    "### 일반화된 설명\n",
    "\n",
    "* k 개의 source(arms), 각 source 로부터 다양한 카테고리(prize)\n",
    "* uncertainty about the stochastic mapping from sources to categories\n",
    "* Our goal is to infer the human’s beliefs about the sources and their preferences over categories\n",
    "* The sources could be blogs or feeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
