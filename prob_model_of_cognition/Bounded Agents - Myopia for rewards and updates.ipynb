{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time inconsistency II\n",
    "\n",
    "## Review of MDP \n",
    "\n",
    "### Tuple of ⟨S,A(s),T(s,a),U(s,a)⟩\n",
    "\n",
    "\n",
    "![](./images/bounded/01.png)\n",
    "![](./images/bounded/02.png)\n",
    "\n",
    "\n",
    "## Formal Model and Implementation of Hyperbolic Discounting\n",
    "\n",
    "* 지연(delay)를 나타낼 수 있는 새로운 변수 추가\n",
    "  * 주관적(subjective)인 시간 지표이다.\n",
    "  * c.f) 일반적인 time-index는 객관적인 시간 지표이다, (ex. timeLeft)\n",
    "* 지연된 보상을 평가(evaluate, prospect)를 할 때 고려한다.\n",
    "* Naive agent의 실수 \n",
    "  * 미레자아가 현재자아처럼 동일한 선호를 가지고 있을 것이라 가정\n",
    "  * t자아는 t+c자아가 자신처럼 delay c 를 가질 것이라고 해석해서 시뮬레이션한다.\n",
    "  * 1/(1+kd)=1/(1+kc)\n",
    "* Sophisticated 의 정정\n",
    "  * t+c 자아는 c=0, 즉 immediate reward로 해석할 것이라고 시뮬레이션한다.\n",
    "  * 1/(1+kd)=1/(1+0)\n",
    "  \n",
    "\n",
    "![](./images/bounded/03.png)\n",
    "\n",
    "![](./images/bounded/09.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of chinese restaurant\n",
    "\n",
    "* Naive Agent의 경우\n",
    "  * 제일 첫번째 위로 갈때는 Veg가 좋아보임 \n",
    "  * 도넛 가게 근처로 갔을 때 도넛 가게로 빨려들어감\n",
    "    * 미래 자아에서 선호가 달라진 증거\n",
    "    * 도넛 가게 안으로 들어갈 때 받는 즉각적 보상이 매우 크고(냄새가 너무 좋다), 먹은 후 받게 되는 음의 보상(후회)는 감쇠한다.  \n",
    "* Sophistecated Agent의 경우 \n",
    "  * 먹고 난 후의 후회 보상을 discount하지 않고 계산하므로, Veg가 들어갈때보상(-10)+먹은후보상(+20)=10이 가장 dominant\n",
    "  * Veg의 먹고 난 후의 보상을 줄이면(10) Veg로 안갈 수도 있다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Procrastinating on a task\n",
    "\n",
    "* 할일 미루기\n",
    "  * deadline 10일 남음\n",
    "  * 한다면 하루종일 걸리고 cost가 든다.\n",
    "  * task를 deadline 내에 완료하기만 한다면 충분한 보상이 있다. \n",
    "  * 이 보상은 deadline이 가까워지면 줄어들어서, 되도록 빨리 끝내는게 유리 \n",
    "* 최적화된 행동은 즉시 끝내는 것이나, 대부분의 사람들이 미루기를 한다. (systematically bias)\n",
    "* naive agent의 행동\n",
    "  * hyperbolic discount는 내일로 미루는 것을 더 선호케 함\n",
    "  * 더군다나 naive는 내일의 self가 이것에 동의할 것으로 오판, 즉 내일의 나는 task를 하겠지..\n",
    "  * 실제로는 내일이 되면 preference가 달라져서, 다시 그 다음 내일로 미루는 것을 선호\n",
    "  * 결국, deadline까지 가서야 일을 마친다.\n",
    "\n",
    "![](./images/bounded/04.png)\n",
    "\n",
    "\n",
    "* 실험 - discount k에 따라서 언제 일을 하는지\n",
    "  * 값이 작으면 delayed 보상의 감쇠를 작게 보므로 최대한 일을 빨리 한다. \n",
    "  * 값이 크면, delayed 보상을 크게 봐서, 심지어 데드라인 전날에도 즉각적 보상(-w)와 감쇠된 보상을 저울질하다가 그냥 안한다\n",
    "    * 시험 전날에, 벼락치기 하는 고통과 예상되는 낮은 점수 * 감쇠율을 비교하다고 그냥 놀아버림~\n",
    "\n",
    "![](./images/bounded/10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bounded Agents-- Myopia for rewards and updates\n",
    "\n",
    "* 인식적 제약(cognitive bounded)으로 인해 \n",
    "  * suboptimal이지만 \n",
    "  * 빠르고 절약적(frugal)\n",
    "\n",
    "\n",
    "## Reward-myopic Planning: the basic idea\n",
    "\n",
    "* 전통적인 POMDP\n",
    "  * final state의 utility로부터 거꾸로 추정(reason backward)한다. 즉 이전 행동들이 최적의 final state로 이끌었는지 판단 \n",
    "  * 무한대의 시간 고려시(infinite time horizon), 매우 긴 시간의 미래에 도달할 수 있는 모든 미래 상황도 다 고려한다\n",
    "* 보상 근시안적인 agent(reward myopic agent)\n",
    "  * 근시안적으로 협소한 근 미래의 보상만 최적화 목표\n",
    "  * 예를 들어 t=5 이후의 미래만 고려\n",
    "  * 당연히 계산량이 매우 작아진다.\n",
    "* 잘 되는 경우\n",
    "  * 가까운 미래의 최적화가 먼 미래의 최적화로 연계되는 경우\n",
    "  * ex) 쉴 때 잘 쉬기 => 잘 쉬어야 장기적으로 일도 성공?\n",
    "* 잘 안되는 경우, \n",
    "  * ex) 산 정상 오르기 : 한발한발이 매우 고통스러우나, 정상에 올라가면 보상 극대화\n",
    "  * 대응\n",
    "* provide fake shor-term rewards for proxy of long-term rewards\n",
    "  * reward shaping in Reinforcement Learning (Chentanez et al., 2004)\n",
    "  * ex) 열심히 공부해서 먼 미래에 성공한 후의 얻을 보상을 현재 느끼며 흐믓해하기 ?\n",
    "  \n",
    "\n",
    "## how to implement\n",
    "\n",
    "* takes the action that would be optimal if the time-horizon were Cg steps into the future\n",
    "  * cut-off time-horizon, Cg\n",
    "  \n",
    "* 아래처럼 shouldTerminate 함수에서 delay와 Cg의 관계에서 결정\n",
    "\n",
    "![](./images/bounded/12.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example : multi-arm bandit\n",
    "\n",
    "* multi-arm의 경우 단순히 Cg=1(즉각적 보상), 높은 softmax action noise(즉 explore를 적극적으로)만 해도 optimal에 근접\n",
    "  * Kuleshov and Precup, 2014\n",
    " \n",
    "* two-arm case \n",
    "\n",
    "![](./images/bounded/13.png)\n",
    "  \n",
    "\n",
    "![](./images/bounded/11.png)\n",
    "\n",
    "\n",
    "* three-arm case\n",
    "\n",
    "![](./images/bounded/06.png)\n",
    "\n",
    "![](./images/bounded/14.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Myopic Updating: the basic idea\n",
    "\n",
    "* 보상 근시안적인 에이전트\n",
    "  * Cg 이후의 모든 미래 보상을 무시 \n",
    "* 업데이트 근시안적인 에이전트(Update-myopic agent)\n",
    "  * 모든 미래 보상은 고려 \n",
    "  * 대신 belief update를 근시안적으로 함\n",
    "    * ignores the value of belief updates that occur after a cutoff\n",
    "  * explore up to Cg, after then, only exploit\n",
    "  \n",
    "\n",
    "## Remind : POMDP update\n",
    "\n",
    "![](./images/bounded/16.png)\n",
    "\n",
    "![](./images/bounded/15.png)\n",
    "\n",
    "  \n",
    "## Myopic Updating: formal model \n",
    "\n",
    "* we use the idea of delays from the previous chapter\n",
    "* If the future action occurs when delay d exceeds cutoff point\n",
    "  * then the simulated future self does not do a belief update before taking the action\n",
    "\n",
    "![](./images/bounded/07.png)\n",
    "\n",
    "\n",
    "* implementation \n",
    "  * Cg 이후에는 인자로 전달된 observation을 이용하지 않는 것에 주목\n",
    "  \n",
    "![](./images/bounded/17.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Myopic Updating for Bandits\n",
    "\n",
    "* Update myopic agent 는 제한적이자만 Bandit 문제를 잘 푸는 것으로 알려져있다.\n",
    "  * optimal 결과를 잘 예측\n",
    "\n",
    "* POMDP multi-argm 문제\n",
    "  * 두 개의 hypothesis for prior belief \n",
    "\n",
    "![](./images/bounded/08.png)\n",
    "\n",
    "![](./images/bounded/18.png)\n",
    "\n",
    "* number of trials 을 증가시켜가면서 update-myopic 성능과 optimal 성능을 비교해보니 update-myopic은 scalable 하다.\n",
    "\n",
    "![](./images/bounded/19.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Myopic Updating for the Restaurant Search Problem\n",
    "\n",
    "* 레스토랑 퀄러티 prior\n",
    "  * A is 3 (sure)\n",
    "  * B 0~6 \n",
    "  * C 0~6\n",
    "  * D, E 중의 하나는 맛집(5), 하나는 꽝(0), unsure\n",
    "\n",
    "* Optimal agent\n",
    "  * infinite horizon으로 모든 레스토랑으로 찾아가는 것을 시뮬레이션하고 observation을 바탕으로 belief update \n",
    "  * 가장 좋은 퀄러티의 레스토랑(D or E)를 잘 찾아감\n",
    "\n",
    "* update-myopic은 잘 안됨 \n",
    "  * update 제약(bound, Cg=1)가 있음, 즉 레스토랑 앞까지 가봐야 안다 \n",
    "  * 잘 알려진 A쪽으로 감\n",
    "  * Cg를 늘리면 optimal 처럼 D,E쪽으로 방향을 틈"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
