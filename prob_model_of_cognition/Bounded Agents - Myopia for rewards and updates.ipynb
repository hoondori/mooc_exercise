{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time inconsistency II\n",
    "\n",
    "## Review of MDP \n",
    "\n",
    "### Tuple of ⟨S,A(s),T(s,a),U(s,a)⟩\n",
    "\n",
    "\n",
    "![](./images/bounded/01.png)\n",
    "![](./images/bounded/02.png)\n",
    "\n",
    "\n",
    "## Formal Model and Implementation of Hyperbolic Discounting\n",
    "\n",
    "* add a variable for measuring time, the delay\n",
    "  * c.f) TimeLeft \n",
    "* it is subjective\n",
    "* used in evaluating possible future rewards but they are not an independent feature of the decision problem\n",
    "* When evaluating future prospects, they need to keep track of how far ahead in time that reward occurs\n",
    "\n",
    "\n",
    "### How to simulate : naive v.s. sophisticated\n",
    "* navie agent\n",
    "  * The Naive agent at objective time t assumes his future self at objective time t + c shares his time preference\n",
    "  * he simulates the (t+c)-agent as evaluating a reward at time t+c with delay c\n",
    "  * rather than true delay = 0\n",
    "  * with hyperbolic, discounted by 1/1+kc\n",
    "* sophisticated agent\n",
    "  * correctly models his (t+c)-agent future self as evaluating an immediate reward with delay 0\n",
    "  * with hyperbolic, discounted by 1/1+k*0 = 1 \n",
    "\n",
    "![](./images/bounded/03.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of chinese restaurant\n",
    "\n",
    "See example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Procrastinating on a task\n",
    "\n",
    "* 할일 미루기\n",
    "  * deadline 10일 남음\n",
    "  * 한다면 하루종일 걸리고 cost가 든다.\n",
    "  * task를 deadline 내에 완료하기만 한다면 충분한 보상이 있다. \n",
    "  * 이 보상은 deadline이 가까워지면 줄어들어서, 되도록 빨리 끝내는게 유리 \n",
    "* 최적화된 행동은 즉시 끝내는 것이나, 대부분의 사람들이 미루기를 한다. (systematically bias)\n",
    "* naive agent의 행동\n",
    "  * hyperbolic discount는 내일로 미루는 것을 더 선호케 함\n",
    "  * 더군다나 naive는 내일의 self가 이것에 동의할 것으로 오판\n",
    "  * 실제로는 내일이 되면 preference가 달라져서, 다시 그 다음 내일로 미루는 것을 선호\n",
    "  * 결국, deadline까지 가서야 일을 마친다.\n",
    "\n",
    "![](./images/bounded/04.png)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bounded Agents-- Myopia for rewards and updates\n",
    "\n",
    "## Reward-myopic Planning: the basic idea\n",
    "\n",
    "* traditional (PO)MDP\n",
    "  * The optimal POMDP agent reasons backwards from the utility of its final state, judging earlier actions on whether they lead to good final states.\n",
    "* reward myopic agent\n",
    "  * myopically” optimize for near-term rewards\n",
    "  * ex) With 1000 timestep horizion, only up to t=6 optimized\n",
    "  * usually doing much less computation overall\n",
    "* successfuly when\n",
    "  * continually optimizing for the short-term produces good long-term performance\n",
    "  * 잘 안되는 경우, ex) 산 정상 오르기 : 한발한발이 매우 고통스러우나, 정상에 올라가면 보상 극대화\n",
    "\n",
    "## how to implement\n",
    "\n",
    "* takes the action that would be optimal if the time-horizon were Cg steps into the future\n",
    "  * cut-off time-horizon\n",
    "  \n",
    "## Example : multi-arm bandit\n",
    "\n",
    "with Cg=1\n",
    "\n",
    "* two-arm case\n",
    "\n",
    "![](./images/bounded/05.png)\n",
    "\n",
    "\n",
    "* three-arm case\n",
    "\n",
    "![](./images/bounded/06.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Myopic Updating: the basic idea\n",
    "\n",
    "* The Reward-myopic agent \n",
    "  * ignores rewards that occur after its myopic cutoff\n",
    "* “Update-myopic agent”,\n",
    "  * takes into account all future rewards but ignores the value of belief updates that occur after a cutoff\n",
    "  * explore up to Cg, after then, only exploit\n",
    "  * like the Naive hyperbolic discounter, has an incorrect model of their future self\n",
    "* Myopic Updating does not work well for POMDPs in general.\n",
    "  * ???\n",
    "  \n",
    "## Myopic Updating: formal model \n",
    "\n",
    "* we use the idea of delays from the previous chapter\n",
    "* If the future action occurs when delay d exceeds cutoff point\n",
    "  * then the simulated future self does not do a belief update before taking the action\n",
    "\n",
    "![](./images/bounded/07.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Myopic Updating for Bandits\n",
    "\n",
    "![](./images/bounded/08.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Myopic Updating for the Restaurant Search Problem\n",
    "\n",
    "* How does the Update-myopic agent fail? \n",
    "  * 가까이 가보기 전까진 좋은지 안 좋은지 알수 없다. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
