{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning to Learn MDPs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "---\n",
    "* 이전 챕터들에서는 에이전트는 세상에 대한 지식이 풍부\n",
    " * 상태 천이 정보, utility function\n",
    "* POMDP 셋팅에서도 hidden 상태가 주어지면 세상이 어떻게 동작하는지를 이해\n",
    "* 반면 이번 챕터에서는 \n",
    " * 세상의 구조(structure of world)을 잘 모르는 상황에서도 \n",
    " * RL 기법을 이용해서 Agent가 결국 최적의 정책에 도달하는 것을 보인다.\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning for Bandits\n",
    "\n",
    "\n",
    "### k-Multi-arm bandits 문제\n",
    "* 각 time-step 마다 k개의 서로 다른 옵션(액션)이 주어지고 이 중 하나를 선택\n",
    "* 각 선택에 대해 보상이 stationary 확률 분포에 의해 주어진다. \n",
    " * 슬롯 머신에서 k개의 레버가 있고, \n",
    " * 각 레버마다 일종의 stationary bernoulli distribution을 가지고, \n",
    " * 이 보상 분포에서 sample 해서 보상을 꺼낸다.\n",
    "  * ex) Bernoulli(0.7)\n",
    "  \n",
    "![](./images/rl/1.png)\n",
    "  \n",
    "* single-state이고 hidden\n",
    " * 갬블러는 각 레버의 보상 분포를 정확히 알지 못하고(hidden), 추정(updateBelief)한다\n",
    " * ex. (0.7, 0.8) or ( 0.7, 0.2 )\n",
    " \n",
    "![](./images/rl/2.png) \n",
    "![](./images/rl/4.png)\n",
    "![](./images/rl/3.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax Greedy Agent\n",
    "\n",
    "** greedy agent **\n",
    "* expected reward를 최대로 하는 action을 선택\n",
    "\n",
    "** with softmax noise **\n",
    "* 때때로 탐험(exploration)을 위해서 랜덤 action을 취함\n",
    "\n",
    "\n",
    "![](./images/rl/5.png)\n",
    "\n",
    "* 참고) factor 함수는 log-probability of expectation에 additive bias를 준다..\n",
    " * alpha값이 작으면 더 noise하게 된다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습 결과 평가\n",
    "\n",
    "* true state(즉 bandit별 true reward bernoulli dist) 와 모든 belief를 비교해서 누적\n",
    "* cumulative regret\n",
    "\n",
    "![](./images/rl/6.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 질문\n",
    "\n",
    "#### 낙관적 prior를 가진 경우 탐험을 적게 하면?\n",
    "* 각 bandit에 대해서 보상을 잘 줄거라고 낙관적 priorBelief 형성. ex) (0.7, 0.9 ) \n",
    "* 탐험을 적게 하자.\n",
    " * softmax noise low ( alpah large )\n",
    "* true 값이 (0.5, 0.6) 인 상황에서 위의 에이전트의 성능은? \n",
    "\n",
    "![](./images/rl/7.png)\n",
    "\n",
    "\n",
    "#### 거꾸로 비관적인 prior를 가진 경우 탐험을 적게 하면?\n",
    "* ex) (0.2, 0.3)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Posterior Sampling\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "참고\n",
    "* Machine learning - Bayesian optimization and multi-armed bandits\n",
    " * https://youtu.be/vz3D36VXefI\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
