{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Markov Model\n",
    "\n",
    "## a deep probabilistic model for sequential data\n",
    "\n",
    "* build a latent variable model in which the variability and temporal structure of the observations is controlled by the dynamics of the latent variables\n",
    "* Markov model, in which we have a chain of latent variables, with each latent variable in the chain conditioned on the previous latent variable\n",
    "* the transition probabilities governing the dynamics of the latent variables as well as the the emission probabilities that govern how the observations are generated by the latent dynamics to be parameterized by (non-linear) neural networks\n",
    "\n",
    "![](./images/dmm/01.png)\n",
    "\n",
    "\n",
    "* neural network based model\n",
    "  * The solid black squares represent non-linear functions parameterized by neural networks\n",
    "  * black squares appear in two different places: in between pairs of latents and in between latents and observations\n",
    "  * we can freely choose the dimension of the latent space to suit the problem at hand\n",
    "\n",
    "* For the music example\n",
    "  * for state transition, we choose (conditional) gaussian distributions with diagonal covariances\n",
    "  * For  observation likelihoods, the bernoulli distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian State Space Models\n",
    "\n",
    "![](./images/dmm/02.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Markov Models\n",
    "\n",
    "![](./images/dmm/03.png)\n",
    "![](./images/dmm/04.png)\n",
    "![](./images/dmm/05.png)\n",
    "![](./images/dmm/06.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Gated Transition and the Emitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import pyro\n",
    "from pyro.distributions import Normal\n",
    "from pyro.infer import SVI\n",
    "from pyro.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Emitter(nn.Module):\n",
    "    \"\"\"\n",
    "    Parameterizes the bernoulli observation likelihood p(x_t | z_t)\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, z_dim, emission_dim):\n",
    "        super(Emitter, self).__init__()\n",
    "        \n",
    "        # initialize the three linear transformations used in the neural network\n",
    "        self.lin_z_to_hidden = nn.Linear(z_dim, emission_dim)\n",
    "        self.lin_hidden_to_hidden = nn.Linear(emission_dim, emission_dim)\n",
    "        self.lin_hidden_to_input = nn.Linear(emission_dim, input_dim)\n",
    "        \n",
    "        # initialize the two non-linearities used in the neural network\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, z_t):\n",
    "        \"\"\"\n",
    "        Given the latent z at a particular time step t we return the vector of\n",
    "        probabilities `ps` that parameterizes the bernoulli distribution p(x_t|z_t)\n",
    "        \"\"\"\n",
    "        h1 = self.relu(self.lin_z_to_hidden(z_t))\n",
    "        h2 = self.relu(self.lin_hidden_to_hidden(h1))\n",
    "        ps = self.sigmoid(self.lin_hidden_to_input(h2))\n",
    "        return ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GatedTransition(nn.Module):\n",
    "    \"\"\"\n",
    "    Parameterizes the gaussian latent transition probability p(z_t | z_{t-1})\n",
    "    See section 5 in the reference for comparison.\n",
    "    \"\"\"\n",
    "    def __init__(self, z_dim, transition_dim):\n",
    "        super(GatedTransition, self).__init__()\n",
    "        \n",
    "        # initialize the six linear transformations used in the neural network\n",
    "        self.lin_gate_z_to_hidden = nn.Linear(z_dim, transition_dim)\n",
    "        self.lin_gate_hidden_to_z = nn.Linear(transition_dim, z_dim)\n",
    "        self.lin_proposed_mean_z_to_hidden = nn.Linear(z_dim, transition_dim)\n",
    "        self.lin_proposed_mean_hidden_to_z = nn.Linear(transition_dim, z_dim)\n",
    "        self.lin_sig = nn.Linear(z_dim, z_dim)\n",
    "        self.lin_z_to_mu = nn.Linear(z_dim, z_dim)\n",
    "        \n",
    "        # modify the default initialization of lin_z_to_mu\n",
    "        # so that it's starts out as the identity function\n",
    "        self.lin_z_to_mu.weight.data = torch.eye(z_dim)\n",
    "        self.lin_z_to_mu.bias.data = torch.zeros(z_dim)\n",
    "        \n",
    "        # initialize the three non-linearities used in the neural network\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.softplus = nn.Softplus()\n",
    "\n",
    "    def forward(self, z_t_1):\n",
    "        \"\"\"\n",
    "        Given the latent z_{t-1} corresponding to the time step t-1\n",
    "        we return the mean and sigma vectors that parameterize the\n",
    "        (diagonal) gaussian distribution p(z_t | z_{t-1})\n",
    "        \"\"\"\n",
    "        # compute the gating function and one minus the gating function\n",
    "        gate_intermediate = self.relu(self.lin_gate_z_to_hidden(z_t_1))\n",
    "        gate = self.sigmoid(self.lin_gate_hidden_to_z(gate_intermediate))\n",
    "        one_minus_gate = ng_ones(gate.size()).type_as(gate) - gate\n",
    "        \n",
    "        # compute the 'proposed mean'\n",
    "        proposed_mean_intermediate = self.relu(self.lin_proposed_mean_z_to_hidden(z_t_1))\n",
    "        proposed_mean = self.lin_proposed_mean_hidden_to_z(proposed_mean_intermediate)\n",
    "        \n",
    "        # assemble the actual mean used to sample z_t, which mixes a linear transformation\n",
    "        # of z_{t-1} with the proposed mean modulated by the gating function\n",
    "        mu = one_minus_gate * self.lin_z_to_mu(z_t_1) + gate * proposed_mean\n",
    "        \n",
    "        # compute the sigma used to sample z_t, using the proposed mean from above as input\n",
    "        # the softplus ensures that sigma is positive\n",
    "        sigma = self.softplus(self.lin_sig(self.relu(proposed_mean)))\n",
    "        \n",
    "        # return mu, sigma which can be fed into Normal\n",
    "        return mu, sigma\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational inference on DMM\n",
    "\n",
    "### Non-sequential 에 대한 VI \n",
    "\n",
    "![](./images/dmm/07.png)\n",
    "\n",
    "### Sequential 에 대한 VI\n",
    "\n",
    "![](./images/dmm/08.png)\n",
    "\n",
    "![](./images/dmm/09.png)\n",
    "\n",
    "![](./images/dmm/10.png)\n",
    "\n",
    "![](./images/dmm/11.png)\n",
    "\n",
    "### Lower Bound\n",
    "\n",
    "![](./images/dmm/12.png)\n",
    "\n",
    "### Learn by stochastic gradient\n",
    "\n",
    "![](./images/dmm/13.png)\n",
    "\n",
    "![](./images/dmm/14.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structure Inference Network\n",
    "\n",
    "![](./images/dmm/15.png)\n",
    "\n",
    "![](./images/dmm/16.png)\n",
    "\n",
    "![](./images/dmm/17.png)\n",
    "\n",
    "![](./images/dmm/18.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Combiner(nn.Module):\n",
    "    \"\"\"\n",
    "    Parameterizes q(z_t | z_{t-1}, x_{t:T}), which is the basic building block\n",
    "    of the guide (i.e. the variational distribution). The dependence on x_{t:T} is\n",
    "    through the hidden state of the RNN (see the pytorch module `rnn` below)\n",
    "    \"\"\"\n",
    "    def __init__(self, z_dim, rnn_dim):\n",
    "        super(Combiner, self).__init__()\n",
    "        # initialize the three linear transformations used in the neural network\n",
    "        self.lin_z_to_hidden = nn.Linear(z_dim, rnn_dim)\n",
    "        self.lin_hidden_to_mu = nn.Linear(rnn_dim, z_dim)\n",
    "        self.lin_hidden_to_sigma = nn.Linear(rnn_dim, z_dim)\n",
    "        # initialize the two non-linearities used in the neural network\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.softplus = nn.Softplus()\n",
    "\n",
    "    def forward(self, z_t_1, h_rnn):\n",
    "        \"\"\"\n",
    "        Given the latent z at at a particular time step t-1 as well as the hidden\n",
    "        state of the RNN h(x_{t:T}) we return the mean and sigma vectors that\n",
    "        parameterize the (diagonal) gaussian distribution q(z_t | z_{t-1}, x_{t:T})\n",
    "        \"\"\"\n",
    "        # combine the rnn hidden state with a transformed version of z_t_1\n",
    "        h_combined = 0.5 * (self.tanh(self.lin_z_to_hidden(z_t_1)) + h_rnn)\n",
    "        # use the combined hidden state to compute the mean used to sample z_t\n",
    "        mu = self.lin_hidden_to_mu(h_combined)\n",
    "        # use the combined hidden state to compute the sigma used to sample z_t\n",
    "        sigma = self.softplus(self.lin_hidden_to_sigma(h_combined))\n",
    "        # return mu, sigma which can be fed into Normal\n",
    "        return mu, sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guide "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def guide(self, mini_batch, mini_batch_reversed, mini_batch_mask,\n",
    "          mini_batch_seq_lengths, annealing_factor=1.0):\n",
    "\n",
    "    # this is the number of time steps we need to process in the mini-batch\n",
    "    T_max = mini_batch.size(1)\n",
    "    # register all pytorch (sub)modules with pyro\n",
    "    pyro.module(\"dmm\", self)\n",
    "\n",
    "    # if on gpu we need the fully broadcast view of the rnn initial state\n",
    "    # to be in contiguous gpu memory\n",
    "    h_0_contig = self.h_0 if not self.use_cuda \\\n",
    "        else self.h_0.expand(1, mini_batch.size(0), self.rnn.hidden_size).contiguous()\n",
    "    # push the observed x's through the rnn;\n",
    "    # rnn_output contains the hidden state at each time step\n",
    "    rnn_output, _ = self.rnn(mini_batch_reversed, h_0_contig)\n",
    "    # reverse the time-ordering in the hidden state and un-pack it\n",
    "    rnn_output = poly.pad_and_reverse(rnn_output, mini_batch_seq_lengths)\n",
    "    # set z_prev = z_q_0 to setup the recursive conditioning in q(z_t |...)\n",
    "    z_prev = self.z_q_0\n",
    "\n",
    "    # sample the latents z one time step at a time\n",
    "    for t in range(1, T_max + 1):\n",
    "        # get the parameters for the distribution q(z_t | z_{t-1}, x_{t:T})\n",
    "        z_mu, z_sigma = self.combiner(z_prev, rnn_output[:, t - 1, :])\n",
    "        # sample z_t from the distribution q(z_t|...)\n",
    "        z_t = pyro.sample(\"z_%d\" % t, dist.Normal, z_mu, z_sigma,\n",
    "                          log_pdf_mask=annealing_factor * mini_batch_mask[:, t - 1:t])\n",
    "        # the latent sampled at this time step will be conditioned upon in the next time step\n",
    "        # so keep track of it\n",
    "        z_prev = z_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pyro Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def model(self, mini_batch, mini_batch_reversed, mini_batch_mask,\n",
    "          mini_batch_seq_lengths, annealing_factor=1.0):\n",
    "\n",
    "    # this is the number of time steps we need to process in the mini-batch\n",
    "    T_max = mini_batch.size(1)\n",
    "\n",
    "    # register all pytorch (sub)modules with pyro\n",
    "    pyro.module(\"dmm\", self)\n",
    "\n",
    "    # set z_prev = z_0 to setup the recursive conditioning\n",
    "    z_prev = self.z_0\n",
    "\n",
    "    # sample the latents z and observed x's one time step at a time\n",
    "    for t in range(1, T_max + 1):\n",
    "        # the next three lines of code sample z_t ~ p(z_t | z_{t-1})\n",
    "        # first compute the parameters of the diagonal gaussian distribution p(z_t | z_{t-1})\n",
    "        z_mu, z_sigma = self.trans(z_prev)\n",
    "        \n",
    "        # then sample z_t according to dist.Normal(z_mu, z_sigma)\n",
    "        z_t = pyro.sample(\"z_%d\" % t, dist.Normal, z_mu, z_sigma,\n",
    "                          log_pdf_mask=annealing_factor * mini_batch_mask[:, t - 1:t])\n",
    "\n",
    "        # compute the probabilities that parameterize the bernoulli likelihood\n",
    "        emission_probs_t = self.emitter(z_t)\n",
    "        \n",
    "        # the next statement instructs pyro to observe x_t according to the\n",
    "        # bernoulli distribution p(x_t|z_t)\n",
    "        pyro.observe(\"obs_x_%d\" % t, dist.bernoulli, mini_batch[:, t - 1, :],\n",
    "                     emission_probs_t,\n",
    "                     log_pdf_mask=mini_batch_mask[:, t - 1:t])\n",
    "        \n",
    "        # the latent sampled at this time step will be conditioned upon\n",
    "        # in the next time step so keep track of it\n",
    "        z_prev = z_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packaging the Model and Guide as a Pytorch Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DMM(nn.Module):\n",
    "    \"\"\"\n",
    "    This pytorch Module encapsulates the model as well as the\n",
    "    variational distribution (the guide) for the Deep Markov Model\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=88, z_dim=100, emission_dim=100,\n",
    "                 transition_dim=200, rnn_dim=600, rnn_dropout_rate=0.0,\n",
    "                 num_iafs=0, iaf_dim=50, use_cuda=False):\n",
    "        super(DMM, self).__init__()\n",
    "        # instantiate pytorch modules used in the model and guide below\n",
    "        self.emitter = Emitter(input_dim, z_dim, emission_dim)\n",
    "        self.trans = GatedTransition(z_dim, transition_dim)\n",
    "        self.combiner = Combiner(z_dim, rnn_dim)\n",
    "        self.rnn = nn.RNN(input_size=input_dim, hidden_size=rnn_dim, nonlinearity='relu',\n",
    "                          batch_first=True, bidirectional=False, num_layers=1, dropout=rnn_dropout_rate)\n",
    "\n",
    "        # define a (trainable) parameters z_0 and z_q_0 that help define the probability\n",
    "        # distributions p(z_1) and q(z_1)\n",
    "        # (since for t = 1 there are no previous latents to condition on)\n",
    "        self.z_0 = nn.Parameter(torch.zeros(z_dim))\n",
    "        self.z_q_0 = nn.Parameter(torch.zeros(z_dim))\n",
    "        # define a (trainable) parameter for the initial hidden state of the rnn\n",
    "        self.h_0 = nn.Parameter(torch.zeros(1, 1, rnn_dim))\n",
    "\n",
    "        self.use_cuda = use_cuda\n",
    "        # if on gpu cuda-ize all pytorch (sub)modules\n",
    "        if use_cuda:\n",
    "            self.cuda()\n",
    "\n",
    "    # the model p(x_{1:T} | z_{1:T}) p(z_{1:T})\n",
    "\n",
    "    def model(self, mini_batch, mini_batch_reversed, mini_batch_mask,\n",
    "              mini_batch_seq_lengths, annealing_factor=1.0):\n",
    "\n",
    "        # this is the number of time steps we need to process in the mini-batch\n",
    "        T_max = mini_batch.size(1)\n",
    "\n",
    "        # register all pytorch (sub)modules with pyro\n",
    "        pyro.module(\"dmm\", self)\n",
    "\n",
    "        # set z_prev = z_0 to setup the recursive conditioning\n",
    "        z_prev = self.z_0\n",
    "\n",
    "        # sample the latents z and observed x's one time step at a time\n",
    "        for t in range(1, T_max + 1):\n",
    "            # the next three lines of code sample z_t ~ p(z_t | z_{t-1})\n",
    "            # first compute the parameters of the diagonal gaussian distribution p(z_t | z_{t-1})\n",
    "            z_mu, z_sigma = self.trans(z_prev)\n",
    "\n",
    "            # then sample z_t according to dist.Normal(z_mu, z_sigma)\n",
    "            z_t = pyro.sample(\"z_%d\" % t, dist.Normal, z_mu, z_sigma,\n",
    "                              log_pdf_mask=annealing_factor * mini_batch_mask[:, t - 1:t])\n",
    "\n",
    "            # compute the probabilities that parameterize the bernoulli likelihood\n",
    "            emission_probs_t = self.emitter(z_t)\n",
    "\n",
    "            # the next statement instructs pyro to observe x_t according to the\n",
    "            # bernoulli distribution p(x_t|z_t)\n",
    "            pyro.observe(\"obs_x_%d\" % t, dist.bernoulli, mini_batch[:, t - 1, :],\n",
    "                         emission_probs_t,\n",
    "                         log_pdf_mask=mini_batch_mask[:, t - 1:t])\n",
    "\n",
    "            # the latent sampled at this time step will be conditioned upon\n",
    "            # in the next time step so keep track of it\n",
    "            z_prev = z_t\n",
    "\n",
    "    # the guide q(z_{1:T} | x_{1:T}) (i.e. the variational distribution)\n",
    "    def guide(self, mini_batch, mini_batch_reversed, mini_batch_mask,\n",
    "              mini_batch_seq_lengths, annealing_factor=1.0):\n",
    "\n",
    "        # this is the number of time steps we need to process in the mini-batch\n",
    "        T_max = mini_batch.size(1)\n",
    "        # register all pytorch (sub)modules with pyro\n",
    "        pyro.module(\"dmm\", self)\n",
    "\n",
    "        # if on gpu we need the fully broadcast view of the rnn initial state\n",
    "        # to be in contiguous gpu memory\n",
    "        h_0_contig = self.h_0 if not self.use_cuda \\\n",
    "            else self.h_0.expand(1, mini_batch.size(0), self.rnn.hidden_size).contiguous()\n",
    "        # push the observed x's through the rnn;\n",
    "        # rnn_output contains the hidden state at each time step\n",
    "        rnn_output, _ = self.rnn(mini_batch_reversed, h_0_contig)\n",
    "        # reverse the time-ordering in the hidden state and un-pack it\n",
    "        rnn_output = poly.pad_and_reverse(rnn_output, mini_batch_seq_lengths)\n",
    "        # set z_prev = z_q_0 to setup the recursive conditioning in q(z_t |...)\n",
    "        z_prev = self.z_q_0\n",
    "\n",
    "        # sample the latents z one time step at a time\n",
    "        for t in range(1, T_max + 1):\n",
    "            # get the parameters for the distribution q(z_t | z_{t-1}, x_{t:T})\n",
    "            z_mu, z_sigma = self.combiner(z_prev, rnn_output[:, t - 1, :])\n",
    "            # sample z_t from the distribution q(z_t|...)\n",
    "            z_t = pyro.sample(\"z_%d\" % t, dist.Normal, z_mu, z_sigma,\n",
    "                              log_pdf_mask=annealing_factor * mini_batch_mask[:, t - 1:t])\n",
    "            # the latent sampled at this time step will be conditioned upon in the next time step\n",
    "            # so keep track of it\n",
    "            z_prev = z_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Variational Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'input_dim' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-c8f3168b5ec7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# instantiate the dmm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m dmm = DMM(input_dim, z_dim, emission_dim, transition_dim, rnn_dim,\n\u001b[0m\u001b[1;32m      3\u001b[0m           args.rnn_dropout_rate, args.num_iafs, args.iaf_dim, args.cuda)\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# setup optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'input_dim' is not defined"
     ]
    }
   ],
   "source": [
    "# instantiate the dmm\n",
    "dmm = DMM(input_dim, z_dim, emission_dim, transition_dim, rnn_dim,\n",
    "          args.rnn_dropout_rate, args.num_iafs, args.iaf_dim, args.cuda)\n",
    "\n",
    "# setup optimizer\n",
    "adam_params = {\"lr\": args.learning_rate, \"betas\": (args.beta1, args.beta2),\n",
    "               \"clip_norm\": args.clip_norm, \"lrd\": args.lr_decay,\n",
    "               \"weight_decay\": args.weight_decay}\n",
    "optimizer = ClippedAdam(adam_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup inference algorithm\n",
    "svi = SVI(dmm.model, dmm.guide, optimizer, \"ELBO\", trace_graph=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
