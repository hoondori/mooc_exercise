{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review on discrete distribution\n",
    "---\n",
    "\n",
    "## Bernoulli\n",
    "### 1 try coin toss, being head\n",
    "![](./images/ch12/80.png)\n",
    "\n",
    "## Binomial\n",
    "### n try coin toss, count # of head\n",
    "![](./images/ch12/79.png)\n",
    "\n",
    "## Multinomial\n",
    "### n try dice toss, count each side\n",
    "![](./images/ch12/81.png)\n",
    "\n",
    "## Muitinoulli\n",
    "### 1 try dice toss, being one of side\n",
    "![](./images/ch12/82.png)\n",
    "![](./images/ch12/84.png)\n",
    "\n",
    "## Totallly\n",
    "\n",
    "![](./images/ch12/83.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review on SoftMax\n",
    "---\n",
    "\n",
    "* SoftMax is exp andThen normalize\n",
    "* unnormalized score to normalized probabilities\n",
    "\n",
    "![](./images/ch12/85.png)\n",
    "![](./images/ch12/86.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review on Dirichilet distribution and prob. simplex\n",
    "---\n",
    "\n",
    "## ex) 3-side dice\n",
    "\n",
    "![](./images/ch12/88.png)\n",
    "![](./images/ch12/89.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12.4 PCA for categorical data\n",
    "---\n",
    "\n",
    "* observed data is categorical rather than real-valued\n",
    "* note) Ch11, dealt with latent variable is not-real-valued\n",
    "* R 개의 dice를 동시에 던지기 \n",
    "\n",
    "![](./images/ch12/87.png)\n",
    "\n",
    "\n",
    "## example, dice is 3-sided, W1 induce distribution on 3-dim simplex\n",
    "\n",
    "![](./images/ch12/90.png)\n",
    "\n",
    "## posterior inference : E[z|y]\n",
    "* visualization of high-dim categorical data\n",
    "\n",
    "![](./images/ch12/91.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review on entropy and mutual information\n",
    "---\n",
    "\n",
    "## entroy\n",
    "* measure of uncertainty\n",
    "* average number of bits needed to encode data coming from a source with distribution p when we use true model\n",
    "\n",
    "![](./images/ch12/96.png)\n",
    "![](./images/ch12/97.png)\n",
    "\n",
    "## Cross entropy\n",
    "* average number of bits needed to encode data coming from a source with distribution p when we use model q\n",
    "\n",
    "![](./images/ch12/98.png)\n",
    "\n",
    "## KL divergence ( relative entropy)\n",
    "* difference between two dist.\n",
    "\n",
    "![](./images/ch12/99.png)\n",
    "\n",
    "\n",
    "## Mutual information\n",
    "\n",
    "* limited measure of dependency\n",
    "\n",
    "![](./images/ch12/100.png)\n",
    "![](./images/ch12/101.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12.5 PCA for paired and multi-view data\n",
    "---\n",
    "\n",
    "* ex. pair data : movie-user\n",
    "* data fusion : predict one from the other via low-dim bottleneck\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12.5.1 Supervised PCA (latent factor regression)\n",
    "-----\n",
    "\n",
    "* two data set : X, y \n",
    "* Compressing X to predict y\n",
    "\n",
    "<img style=\"float: left;\" src=\"./images/ch12/92.png\" width=\"300\">\n",
    "<img style=\"float: left;\" src=\"./images/ch12/93.png\" width=\"500\">\n",
    "\n",
    "![](./images/ch12/94.png)\n",
    "\n",
    "\n",
    "\n",
    "## Tradeoff between compression and prediction\n",
    "-----\n",
    "\n",
    "![](./images/ch12/102.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12.5.2. Partial least squares (PLS)\n",
    "\n",
    "* X has its own subspace which explain its covariance\n",
    "* more discrinative form of supervised PCA\n",
    "\n",
    "<img style=\"float: left;\" src=\"./images/ch12/104.png\" width=\"500\">\n",
    "\n",
    "![](./images/ch12/105.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12.5.3. Canonical correlation analysis (CCA)\n",
    "-----\n",
    "\n",
    "* Each data has its own latent factors, also share common factors with the other\n",
    "* Application to collaborative filtering : movie-user\n",
    "  * knowledge of who your friends are, as well as the ratings of all other users, should help predict which movies you will like\n",
    "\n",
    "<img style=\"float: left;\" src=\"./images/ch12/95.png\" width=\"500\">\n",
    "\n",
    "![](./images/ch12/103.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12.6 Independent Component Analysis (ICA)\n",
    "---\n",
    "* Cocktail party problem\n",
    "* Blind source seperation (BSS)\n",
    "* x be observed signals, z be source signals\n",
    "* Goal is to infer source signals, given mixed observed signals with noise, p(z|x)\n",
    "\n",
    "![](./images/ch12/106.png)\n",
    "![](./images/ch12/107.png)\n",
    "\n",
    "* Use non-gaussian prior\n",
    "  * note) PCA use gaussian prior\n",
    "* Why? Non-gaussian\n",
    "  * Gaussian suffer from rotation ambiguity\n",
    "  * PCA and ICA both find proper linear subspace, but only ICA find proper rotation\n",
    "\n",
    "![](./images/ch12/108.png)\n",
    "![](./images/ch12/109.png)\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review on Transformation of Random Variables\n",
    "---\n",
    "### Linear transformation\n",
    "\n",
    "![](./images/ch12/111.png)\n",
    "![](./images/ch12/112.png)\n",
    "![](./images/ch12/113.png)\n",
    "\n",
    "\n",
    "### General transformation\n",
    "\n",
    "* If X, Y is discretepmf of Y is sum of probability mass of X\n",
    "\n",
    "![](./images/ch12/114.png)\n",
    "\n",
    "\n",
    "* If continuous\n",
    "\n",
    "![](./images/ch12/115.png)\n",
    "![](./images/ch12/116.png)\n",
    "![](./images/ch12/117.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12.6.1 MLE of ICA\n",
    "---\n",
    "* X be \n",
    "  * whittened ( zero-centered, unit variances )\n",
    "  * noise-free sensors\n",
    "\n",
    "![](./images/ch12/110.png)\n",
    "![](./images/ch12/118.png)\n",
    "![](./images/ch12/119.png)\n",
    "![](./images/ch12/120.png)\n",
    "![](./images/ch12/121.png)\n",
    "\n",
    "* How to solve W with NLL\n",
    "  * gradient-descent (first-order)\n",
    "  * Newtons method (second-order) => FastICA (12.6.2)\n",
    "  * EM => (12.6.3)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review on derivation trick\n",
    "\n",
    "![](./images/ch12/122.png)\n",
    "![](./images/ch12/123.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12.6.2 FastICA for MLE of ICA\n",
    "\n",
    "* Kwown prior, G(z)\n",
    "\n",
    "![](./images/ch12/124.png)\n",
    "![](./images/ch12/125.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12.6.2.1 Modeling the source densities\n",
    "\n",
    "* How to prepare prior, G(z)\n",
    "* Gaussian prior suffer from unidentifiability\n",
    "* Non-gaussian assumptions\n",
    "  * Super-Gaussian (leptokurtic) : big spike at mean, heavy long tails\n",
    "  * Sub-Gaussian(platykurtic) : much flatter than gaussian, ex) uniform\n",
    "  * Skewed\n",
    "  * Laplace\n",
    "  * logstic\n",
    "  \n",
    "![](./images/ch12/126.png)\n",
    "<img style=\"float: left;\" src=\"./images/ch12/127.png\" width=\"500\">\n",
    "<img style=\"float: left;\" src=\"./images/ch12/128.png\" width=\"400\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12.6.3 Using EM for MLE of ICA\n",
    "\n",
    "* Prior as univariate gaussian mixture (non-parametric approach)\n",
    "\n",
    "![](./images/ch12/129.png)\n",
    "![](./images/ch12/130.png)\n",
    "![](./images/ch12/131.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12.6.4 Other estimation principles\n",
    "\n",
    "* we saw MLE-based ICA parameter estimation\n",
    "* any other perspective?\n",
    "\n",
    "## Maximize non-gaussianity\n",
    "* An early approach to ICA was to find a matrix V such that the distribution z = Vx is as far from Gaussian as possible\n",
    "* Note that gaussian has maximun entropy dist.\n",
    "\n",
    "![](./images/ch12/132.png)\n",
    "\n",
    "\n",
    "## Minimize mutual information\n",
    "* minimize this, since we are trying to find independent components\n",
    "\n",
    "![](./images/ch12/133.png)\n",
    "![](./images/ch12/134.png)\n",
    "\n",
    "\n",
    "## Maximize mutual information (infomax)\n",
    "* imagine neural network, x be input, y being output\n",
    "* maximize MI between X, y => maximize dependencies\n",
    "* H(y|x) = const (noise has constant variance)\n",
    "\n",
    "![](./images/ch12/137.png)\n",
    "![](./images/ch12/135.png)\n",
    "![](./images/ch12/136.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
