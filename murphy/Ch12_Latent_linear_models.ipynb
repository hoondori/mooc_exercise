{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 11장 GMM 복습\n",
    "\n",
    "* observed variables are correlated because they arise from a hidden common \"cause\"\n",
    "* advantages\n",
    "  * fewer parameters    \n",
    "  * compressed representation of original data\n",
    "    \n",
    "![](./images/ch12/1.png)\n",
    "![](./images/ch12/2.png)\n",
    "\n",
    "\n",
    "* various latent models\n",
    "\n",
    "![](./images/ch12/3.png)\n",
    "\n",
    "* Mixture of gaussian ( or GMM )\n",
    "  * one-to-many mapping ( i.e single latent variable )\n",
    "  * weighted sum\n",
    "  \n",
    "![mixture model](./images/ch12/7.png)  \n",
    "![mixture model](./images/ch12/6.png)\n",
    "![GMM](./images/ch12/4.png)\n",
    "![example of GMM](./images/ch12/5.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ch 12 Latent linear models \n",
    "\n",
    "## Factor analysis\n",
    "\n",
    "* GMM의 단점 : single latent variable, ordinal value => limited representation power\n",
    "* factor analysis\n",
    "  * multiple variable\n",
    "  * real value\n",
    "\n",
    "![](./images/ch12/8.png)\n",
    "\n",
    "* Size\n",
    "  * N : # of data\n",
    "  * D : observed variable dimension\n",
    "  * L : latent variable dimension\n",
    "  * N >> D >> L\n",
    "\n",
    "* Assumption\n",
    "  * prior, p(z) is gaussian, usually zero-mean, unit covariance\n",
    "  * likelihood, p(x|z) is gaussian, linear combination of latent variables with disturbance term\n",
    "    * cov of disturbance is diagonal, or identitical, or orthonormal\n",
    "    \n",
    "![](./images/ch12/9.png)\n",
    "![](./images/ch12/10.png)\n",
    "\n",
    "![graphical repr of FA](./images/ch12/16.png)\n",
    "  \n",
    "  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review on two MVN manipulation\n",
    "\n",
    "## Given joint MVN, marginal MVNs, find conditionals\n",
    "\n",
    "![](./images/ch12/31.png)\n",
    "\n",
    "## Given marginal MVN, conditional MVN, find inverse conditional MVN\n",
    "\n",
    "\n",
    "![](./images/ch12/32.png)\n",
    "![](./images/ch12/33.png)\n",
    "![](./images/ch12/34.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 12.1.1 FA is low rank parameterization of an MVN\n",
    "\n",
    "* as a way of specifying a joint density model on x using a small number of parameters\n",
    "  * understand correlation of obseved variables in a more economical way\n",
    "  * O(D^2) => O(LD)\n",
    "  \n",
    "![](./images/ch12/11.png)\n",
    "![](./images/ch12/12.png)\n",
    "![](./images/ch12/13.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 12.1.2 Inference of the latent factors\n",
    "\n",
    "* infer latent factors by using bayes' rule\n",
    "  * computation takes O(L^3 + L^2*D)\n",
    "\n",
    "![](./images/ch12/14.png)\n",
    "\n",
    "* visualize and userstand meaning of latent factors\n",
    "  * project basis vector of data onto lower dimension\n",
    "  \n",
    "![](./images/ch12/15.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.1.3 Unidentifiability\n",
    "\n",
    "* Unidentifiable\n",
    "  * ex) X + Y = 3 을 만족하는 (x,y)는 무수히 많다. 즉 특정지을 수 없다.\n",
    "  \n",
    "* Example : Orthogonal rotation matrix muliply factor loading matrix\n",
    "  * same mapping from latent to data\n",
    "  * cannot uniquely identify latents\n",
    "  \n",
    "  ![](./images/ch12/29.png)\n",
    "\n",
    "\n",
    "  \n",
    "  \n",
    "  \n",
    "* 왜 발생하나? free variable이 너무 많아서... \n",
    "* 어떻게 방지하나? contraint를 걸어서 free variable을 줄이면 된다.\n",
    "  * orthonormal matrix R에 의해서 발생한 variable 개수만큼 줄어들면 된다.\n",
    "  * L by L orthonormal matrix => L(L-1)/2 degree of freedom\n",
    "  * FA should have degree of freedom = D + LD + - L(L-1)/2\n",
    "    * this should be less than D(D+1)/2 <- dof of uncontrainted,symmetric covariance matrix\n",
    "    * upper bound on L\n",
    "    * Lmax를 택해도 Non-identifiability\n",
    "    \n",
    "      ![](./images/ch12/30.png)\n",
    "      \n",
    "    \n",
    "* Non-identifiability가 predictive performance를 해치는 것은 아니지만 W를 변화시키므로, latent variable의 해석을 어렵게는 한다.\n",
    "\n",
    "* 더 나은 solution\n",
    "  * W orthonormal\n",
    "  * W lower triangular\n",
    "  * W be sparse (like L1 regularization)\n",
    "  * Choose informative rotation matrix R for better explating latent factors\n",
    "  * Use non-gaussian latent => ICA\n",
    "  \n",
    "  \n",
    "  \n",
    "\n",
    "\n",
    "(TODO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 12.1.4 Mixture of factor analyzers\n",
    "\n",
    "* Remind of mixture of experts in ch11\n",
    "\n",
    "![mixture of experts](./images/ch12/17.png)\n",
    "\n",
    "* FA assume low dimensional linear manifold\n",
    "* XFA assme low dimensional curved manifold => approximated by piecewise linear manifold\n",
    "\n",
    "![mixture of experts](./images/ch12/18.png)\n",
    "\n",
    "![mixture of experts](./images/ch12/19.png)\n",
    "\n",
    "![mixture of experts](./images/ch12/20.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.1.5 EM for factor analysis models\n",
    "\n",
    "### Jensen's inequality \n",
    "\n",
    "![mixture of experts](./images/ch12/21.png)\n",
    "![mixture of experts](./images/ch12/22.png)\n",
    "\n",
    "\n",
    "### EM algorithm\n",
    "\n",
    "![mixture of experts](./images/ch12/23.png)\n",
    "![mixture of experts](./images/ch12/24.png)\n",
    "![mixture of experts](./images/ch12/25.png)\n",
    "![mixture of experts](./images/ch12/26.png)\n",
    "\n",
    "\n",
    "### EM for (M)FA\n",
    "\n",
    "![mixture of experts](./images/ch12/27.png)\n",
    "\n",
    "![mixture of experts](./images/ch12/28.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.1.6 Fitting FA with missing data\n",
    "\n",
    "[TODO]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review on Gaussian dist\n",
    "\n",
    "<img style=\"float: left;\" src=\"./images/ch12/35.png\" width=\"400\">\n",
    "<img style=\"float: left;\" src=\"./images/ch12/36.png\" width=\"400\">\n",
    "<img style=\"float: left;\" src=\"./images/ch12/37.png\" width=\"500\">\n",
    "<img style=\"float: left;\" src=\"./images/ch12/38.png\" width=\"700\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review on eigenvalue, eigenvector\n",
    "\n",
    "* intuition of eigenvalue, eigenvector\n",
    "  * Eigenvectors are the directions along which linear transformation occurs only by scaling\n",
    "  * eigenvalues are the scales along those directions\n",
    "\n",
    "<img style=\"float: left;\" src=\"./images/ch12/45.png\" width=\"500\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review on orthonormal matrix\n",
    "\n",
    "<img style=\"float: left;\" src=\"./images/ch12/53.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review on lagrange multiplier\n",
    "\n",
    "![](./images/ch12/51.png)\n",
    "<img style=\"float: left;\" src=\"./images/ch12/52.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review on singular value decomposition\n",
    "\n",
    "<img style=\"float: left;\" src=\"./images/ch12/54.png\" width=\"400\">\n",
    "<img style=\"float: left;\" src=\"./images/ch12/55.png\" width=\"400\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12.2 Principle component analysis(PC)\n",
    "\n",
    "![](./images/ch12/39.png)\n",
    "\n",
    "## Classical PCA\n",
    "\n",
    "* minimize average reconstruction error\n",
    "\n",
    "![](./images/ch12/40.png)\n",
    "![](./images/ch12/41.png)\n",
    "\n",
    "* matrix-form notation, Frobenius norm for matrix\n",
    "\n",
    "![](./images/ch12/42.png)\n",
    "![](./images/ch12/43.png)\n",
    "\n",
    "* solution : principle component\n",
    "\n",
    "![](./images/ch12/44.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.2.2. Proof of solution of PCA\n",
    "\n",
    "![](./images/ch12/46.png)\n",
    "![](./images/ch12/47.png)\n",
    "![](./images/ch12/48.png)\n",
    "![](./images/ch12/49.png)\n",
    "![](./images/ch12/50.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.2.3 Singular value decomposition (SVD)\n",
    "\n",
    "* PCA by SVD\n",
    "* Economy sized SVD\n",
    "\n",
    "![](./images/ch12/56.png)\n",
    "![](./images/ch12/57.png)\n",
    "![](./images/ch12/58.png)\n",
    "\n",
    "* Connection between SVD and PCA\n",
    "  * PCA is truncated SVD\n",
    "  \n",
    "![](./images/ch12/59.png) \n",
    "![](./images/ch12/60.png)\n",
    "![](./images/ch12/61.png)\n",
    "![](./images/ch12/62.png)\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.2.4 Probabilistic PCA (PPCA)\n",
    "\n",
    "![](./images/ch12/63.png)\n",
    "![](./images/ch12/64.png)\n",
    "\n",
    "\n",
    "\n",
    "## Remind on EM for (M)FA\n",
    "\n",
    "![mixture of experts](./images/ch12/28.png)\n",
    "\n",
    "## 12.2.5. EM for PCA \n",
    "\n",
    "* PCA by eigenvector method\n",
    "* PCA by SVD\n",
    "* PCA by EM over PPCA\n",
    "\n",
    "![](./images/ch12/65.png)\n",
    "![](./images/ch12/66.png)\n",
    "![](./images/ch12/67.png)\n",
    "<img src=\"./images/ch12/68.png\" width=\"700\">\n",
    "\n",
    "* EM is good\n",
    "  * EM can be faster : O(TLND)\n",
    "  * online-EM\n",
    "  * handle missing data\n",
    "  * extends to mixture of PPCA\n",
    "  * extends to variational EM\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 12.3 Choosing the number of latent dimensions\n",
    "\n",
    "\n",
    "### 12.3.1 Model selection for FA/PPCA\n",
    "\n",
    "#### Review on model selection by cross-validation\n",
    "\n",
    "![](./images/ch12/69.png)\n",
    "![](./images/ch12/70.png)\n",
    "![](./images/ch12/71.png)\n",
    "\n",
    "#### By BIC or AIC\n",
    "\n",
    "![](./images/ch12/72.png)\n",
    "\n",
    "#### By Automatic relavancy determination(ARD)\n",
    "\n",
    "we will see 13.7\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.3.2 model selection  for PCA\n",
    "\n",
    "* minimize reconstruction error\n",
    "  * elbow point => sharply drop of eigenvalues\n",
    "  * U shaped on test set => PCA is not generative model, it is compression technique\n",
    "  * generative model usually enjoy baysian occam's razor effect\n",
    "\n",
    "![](./images/ch12/73.png)\n",
    "![](./images/ch12/74.png)\n",
    "![](./images/ch12/75.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 12.3.2.1 Profile likelihood\n",
    "\n",
    "* given eigenvalues, sort and disect into two groups\n",
    "* each group follow gaussian dist.\n",
    "* find proper cutting point where profile log likelihood is maximized\n",
    "\n",
    "![](./images/ch12/76.png)\n",
    "![](./images/ch12/77.png)\n",
    "![](./images/ch12/78.png)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
