{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Sequential Neural Models with Stochastic Layers](https://arxiv.org/abs/1605.07571)\n",
    "\n",
    "\n",
    "요약\n",
    "* 기존 RNN은 sequence 데이터를 잘 모델링하기는 하지만 variability가 좋지 못했다. \n",
    "* Autoencoder에 variability를 강화시킨 variational autoencoder의 approach를 받아들여서\n",
    "* RNN을 State Space Model과 융합하였다.\n",
    "* intractable 문제에 대해서는 variational inference로 대응하였다.\n",
    "\n",
    "\n",
    "\n",
    "20.10.3 Variational Autoencoders\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "시퀀스 데이터(ex. language)를 모델링하는 방법\n",
    "* 전통적으로 Dynamic Bayes Network , ex) Kalman filter, HMM\n",
    "* 최근에는 RNN\n",
    "\n",
    "하지만 RNN은 variability가 약하다. \n",
    "최근 Variational Autoencoder의 사례에서 영감을 얻어 RNN + State model => Variational 을 강화한 RNN\n",
    "\n",
    "사전 지식\n",
    "* Dynamic Bayes Network\n",
    "* Variational Inference, Variational Autoencoder\n",
    "\n",
    "\n",
    "* Lower Bound of logp => KL + l...  <- youtube video capture\n",
    "Lower bound를 최대화하거나 혹은 KL 을 최소화\n",
    "easy                       hard\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Tutorial on Variational Autoencoders](https://arxiv.org/pdf/1606.05908v2.pdf)\n",
    "\n",
    "\n",
    "Generative Model\n",
    "* 고차원에서 변수끼리의 dependency를 알아내는 것, 구조를 이해하는 것\n",
    "* 기 수집된 데이터를 기반으로 해서 아직 관찰되지 않은(하지만 있음직한) 데이터를 자체로 생성하는 것\n",
    "\n",
    "\n",
    "Generating MNIST data\n",
    "* 잠재변수 0-9까지 중 하나를 선택한 후(sample from z), 필기체 그림(x)를 generate\n",
    "* 잠재변수와 모델 파리미터는 generate한 데이터가 Traning Data처럼 되게 학습되어야 한다.\n",
    "\n",
    "(그림)\n",
    "![](./images/snm/01.png)\n",
    "![](./images/snm/02.png)\n",
    "\n",
    "\n",
    "Variational AutoEncoder\n",
    "* 어떻게 잠재변수를 모델링할 것인가? \n",
    " * 그냥 N(0,I)로 하고 powerful한 function approximator를 사용해서 function을 학습하자.\n",
    "* 어떻게 Integral을 할 것인가?\n",
    "  * z에 대한 최대한 많은 sample들에 대해 P(X|z)를 구한 후 평균 => 굉장히 많은 sample 필요해서 비현실적\n",
    "  * 잠재변수 z의 space를 줄이는 것 => P(z|X), encoding\n",
    "    * P(z|X)를 구하는 것은 어려움, 대신 이와 최대한 가까운 Q(z|X)를 구하자 => Variational Inference\n",
    "\n",
    "Variational Inference 풀기\n",
    "* 데이터에 대한 정보를 최대화하는 것 \n",
    "* Q를 잘 선택해서 P(z|X)를 잘 근사화하는 것 => 풀기 어려움\n",
    "* stochastic gradient descent로 아래 식의 우측항을 최대화하는 것\n",
    "\n",
    "![](./images/snm/03.png)\n",
    "![](./images/snm/04.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [A Recurrent Latent Variable Model for Sequential Data]()\n",
    "\n",
    "Sequence data에 대한 generative model을 전통적으로 Dynamic Baysian network(DBN)이 효과적이었다\n",
    "* DBN의 예는 Kalman filter나 HMM이 있다.\n",
    "* DBN은 최근에 RNN 에 밀렸다. \n",
    "* RNN은 DBN에 비해 LSTM이나 GRU와 같은 rich한 internal state 표현력이 풍부하고 nonlineary를 가지는 장점은 있으나\n",
    "  * state의 천이가 결정적(deterministic)하다는 한계로 인해서 expressive power가 DBN보다는 떨어진다.\n",
    "* 그래서 RNN은 매우 구조화되고 variable한 데이터에는 적합하지 않다.\n",
    "  * be an inappropriate way to model the kind of variability observed in highly structured data,\n",
    "\n",
    "\n",
    "Variational RNN\n",
    "* Variational AE에서처럼 잠재변수를 도입해서 variability를 개선시키자.\n",
    "* 모든 timestamp마다 variational ae가 있는 셈\n",
    "\n",
    "![](./images/snm/06.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
