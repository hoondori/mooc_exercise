{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ch 4. Beyond Classical Search\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 4.1 지역 탐색 알고리즘 및 최적화 문제\n",
    "\n",
    "Goal에 도달해서 문제를 푼 것이 중요하지 Goal에 도달한 경로가 중요치 않은 경우도 많다. \n",
    "- ex) 8 queen problem\n",
    "\n",
    "Local search는 하나의 node만을 움직여서 goal에 도달하려고 하며 보통 중간 경로도 보존하지 않는다.\n",
    "\n",
    "Local search는 매우 작은 메모리만을 운용하면서도 매우 넓은 infinte continuous space에서도 적당한 해를 찾을 수 있는 장점이 있다. \n",
    "\n",
    "Local search는 Goal-finding 문제 뿐만 아니라 최적화 문제를 다룰 수 있으며 적당한 목적함수만 정해주면 된다.\n",
    "* 이 경우 goal test나 path cost라는 개념은 없다, \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.1 언덕 오르기 탐색\n",
    "\n",
    "값이 증가하는 방향으로 계속해서 전진해나가는 방법, 꼭대기(peak)에 도달하였을 때 종료\n",
    "\n",
    "![](./images/ch04/01.png)\n",
    "![](./images/ch04/02.png)\n",
    "\n",
    "Greedy Local search이다. 왜냐하면 먼 미래에 대한 고민없이 그 시점에서 좋은 이웃만 탐색하기 때문이다. \n",
    "\n",
    "종종 다음과 같이 갇힌다.\n",
    "- 지역 극대\n",
    "- 연속된 지역 극대 (rigdes)\n",
    "- 대평원 (plateux)\n",
    "\n",
    "Sideway walk\n",
    "- 대평원을 만났을 때 일정정도(ex.100번) 한계를 두고 앞으로 계속 나아간다.\n",
    "- 대평원이 실은 shoulder 에 지나지 않을 것이라는 희망을 가지고\n",
    "\n",
    "연속된 지역 극대 지역에 걸리면 다음 지역 극대까지는 모두 downhill이므로 실질적으로 더 높은 지역 극대로 도달하기가 쉽지 않다.\n",
    "![](./images/ch04/03.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 언덕 오르기 변형\n",
    "\n",
    "** Stochastic 언덕 오르기 **\n",
    "\n",
    "* 여러 개의 오르는 방향 후보가 있을 때, 가장 기울기가 급한 방향으로 무조건 가는 것이 아니라 기울기에 비례해서 확률적으로 정한다.\n",
    "* 수렴은 조금 늦게 되지만 때로는 더 나은 결과를 초래\n",
    "\n",
    "** Random-restart 언덕 오르기 **\n",
    "\n",
    "* 이제까지 알고리즘은 지역 극대가 갇히는 경우 실패로 귀결\n",
    "* random-restart는 출발지(initial state)를 랜덤하게 정해서 goal에 도달할 때까지 시도/실패를 계속하는 방법\n",
    "* 각 시도가 성공할 확률이 p(0.1) 이라면 보통 1/p (10번)만 시도하면 성공한다는 논리\n",
    "* 보통 지역 극대나 ridge, plateux가 적은 문제라면 몇번의 시도끝에 성공할 것이다. \n",
    "* 그러나 보다 복잡한 landscape가 펼쳐진다면 한계가 있다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2 Simulated Anneleaing\n",
    "\n",
    "지역 극소에 갇힌 탐색점을 빠져나갈 수 있도록 흔들어주는 것이다.\n",
    "\n",
    "원래 금속을 단단하게 하기 위해서 매우 고온으로 가열했다가 서서히 식혀줄 때의 상황에서 유래\n",
    "\n",
    "best move(greedy search 관점)을 채택하는 것이 아니라 random move(비록 bad move라도)를 선택하는 것이 핵심\n",
    "\n",
    "그 움직임이 더 나은 상황으로 이동하는 것이라면 항상 accept이지만 그게 아니라면 1보다 작은 확률에 근거해서 채택\n",
    "\n",
    "선택 확률은 그 움직임이 얼마나 상황을 악화시키는지(badness)에 비례해서 낮아진다. \n",
    "\n",
    "선택 확률은 또한 온도에 비례해서 계속 낮아진다. (여기서 온도라면 보통 탐색의 총시간으로 보면, 탐색 초기에는 높고, 점차 시간이 지나면서 낮아진다)\n",
    "\n",
    "![](./images/ch04/04.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.3 Local Beam Search\n",
    "\n",
    "단 하나의 node만 탐색 과정상에 유지한다는 것은 아무리 메모리 제약을 고려하더라도 극단적인 조치이다.\n",
    "\n",
    "반면 Local beam search는 여러 개의 탐색 노드를 유지한다.\n",
    "\n",
    "random-restart와의 차이점\n",
    "- RS는 순차적으로, BS는 병렬로 진행\n",
    "- RS는 탐색끼리 서로 영향을 안주는 독립적, BS는 한쪽 탐색의 유용한 정보가 다른 탐색에 전달되서 도움이 되도록 한다.\n",
    "\n",
    "Stochastic beam search\n",
    "- 일반 beam search에서는 병렬로 탐색이 진행되더라도 시간이 지나면 매우 좁은 지역에 탐색들이 모여드는 집중의 문제(다양성의 부족 문제)가 생김\n",
    "- stochastic 버젼은 random move를 허용함으로써 이러한 다양성 부족의 문제를 극복한다.\n",
    "- 마치 진화 알고리즘과 유사"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.4 진화 알고리즘\n",
    "\n",
    "stochasitc beam search의 변형으로 다음 탐색 노드를 두 개의 부모 노드를 혼합함으로 정한다.\n",
    "\n",
    "자연 선택(natural selection)이라는 관점에서 stochastic beam search는 asexual reproduction이고 GA는 sexual reproduction이다.\n",
    "\n",
    "개념\n",
    "- population ( initial randomly generated states )\n",
    "- fitness function\n",
    "- crossover \n",
    "- mutation\n",
    "\n",
    "![](./images/ch04/05.png)\n",
    "\n",
    "![](./images/ch04/06.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Local search in continuous spaces\n",
    "\n",
    "1차 미분을 이용한 언덕 오르기\n",
    "- 쉽지만 step size를 정하기 쉽지 않다.\n",
    "- line search : 처음에는 step size를 크게, 나중에는 작게\n",
    "\n",
    "![](./images/ch04/07.png)\n",
    "\n",
    "\n",
    "2차 미분을 이용한 언덕 오르기\n",
    "- step size를 구할 필요는 없어졌지만 \n",
    "- Hessian 구하는 시간이 너무 오래 걸린다.\n",
    "\n",
    "![](./images/ch04/08.png)\n",
    "\n",
    "제약 조건이 있는 최적화 문제\n",
    "- linear programming, special case of convex optimization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Searching with nondeterministic actions\n",
    "\n",
    "환경이 partially observable이거나 nondeterministic한 경우 지각(percept) 정보가 매우 중요해진다.\n",
    "\n",
    "그런 환경에서는 percept 정보가 agent에게 각 action 수행시 어떠한 결과(outcome)을 초래할 것인지에 대한 것을 가늠케 해준다.\n",
    "\n",
    "미래의 percept는 미리 결정지어질수가 없고 agent의 미래 action도 이러한 미래의 percept에 의존하게 된다.\n",
    "\n",
    "이 경우 문제에 대한 해결은 단순히 sequence of action이 아니라 contingency plan(or strategy)이다. \n",
    "\n",
    "이것은 어떠한 지각이 주어질 때 어떻게 행동할 것인가에 대한 것이다. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.1 erratic vaccum world\n",
    "\n",
    "초기 상태가 1번이었고 exact vaccum world였다면 action sequence는 [SUCK, RIGHT, SUCK] 이었을 것이다. \n",
    "\n",
    "erratic vaccum world에서는 여기를 청소하라고 시켜도 가끔씩 옆방을 청소하는 오류가 발생한다. \n",
    "\n",
    "이로 인해 state 1에 청소명령을 내리면 state 5번만 되는 것이 아니라 state 7번이 되기도 한다.\n",
    "\n",
    "그래서 contingency plan은 [Suck if State = 5, then [Right,Suck] else []] 처럼 nested if-then-else 가 된다.\n",
    "\n",
    "즉 단순 sequence가 아니라 tree처럼 되는 것이다.\n",
    "\n",
    "![](./images/ch04/09.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.2 AND-OR search trees\n",
    "\n",
    "deterministic한 환경에서는 agent의 선택에 의해 다음 state 후보군이 추스려진다. 이런 것을 OR nodes라고 부른다.\n",
    "\n",
    "반면 nondeterministic한 환경에서는 agent의 action으로 인한 것 뿐만 아니라 환경 자체의 선택에 의해서도 branch가 생긴다. 이를 AND node라고 하자.\n",
    "\n",
    "이렇게 Agent의 선택 branch, 환경의 선택 branch가 연속되는 것을 AND-OR Search Tree라고 한다.\n",
    "\n",
    "![](./images/ch04/10.png)\n",
    "\n",
    "AND-OR tree의 solution\n",
    "- contingency plan 상에서 AND branch의 leaf에는 goal이 존재\n",
    "- OR 에서는 단 하나의 action만 선택\n",
    "- AND branch에서는 모든 outcome이 표시\n",
    "\n",
    "알고리즘\n",
    "- AND-OR을 반복\n",
    "- Path (루트로부터 해당 node까지) 상에 이미 방문했던 state가 있다면 fail, 즉 cycle은 fail\n",
    "- 이러한 cycle 방지 전략이 있어서 finite한 상태 공간에서 이 알고리즘은 항상 끝난다.\n",
    "\n",
    "![](./images/ch04/11.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.3 Try, try again\n",
    "\n",
    "Slippery vaccum world\n",
    "- 가끔 action을 해도 실패하는 상황\n",
    "- 예를 들어 state 1에서 오른쪽 이동했을 때 Result set은 {1,2}\n",
    "\n",
    "Cyclic solution\n",
    "- 실제 action이 성공할 때까지 계속 시도\n",
    "- label을 매겨서 goto label\n",
    "\n",
    "![](./images/ch04/13.png)\n",
    "\n",
    "![](./images/ch04/12.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Searching with partial observations\n",
    "\n",
    "partial observable\n",
    "- agent가 특정 상태에 확실히 있는 것이 아니라\n",
    "- agent가 여러 상태에 있을 수 있는 상황 (possible states)\n",
    "- agent가 자신이 머물르고 있다고 믿는 상태 ( belief states )\n",
    "\n",
    "### 4.4.1 Search with no observation\n",
    "\n",
    "conformant 혹은 sensorless\n",
    "- agent의 지각이 어떤 정보도 제공해 주지 않는 상황\n",
    "\n",
    "sensorless 문제의 해결책\n",
    "- physical state를 탐색하기 보다는 belief state를 탐색\n",
    "- belief state 내에서는 오히려 fully observable한 상황이 된다.??\n",
    "- solution은 contingency plan이 아니라 단순한 sequence of action이 된다.\n",
    "- 왜냐면 action 후 지각은 완벽히 예측가능하므로(completely predictable)\n",
    "\n",
    "physical state problem P가 주어졌을 때 이를 완벽히 belief state problem으로 변환시킬 수 있다.\n",
    "\n",
    "![](./images/ch04/14.png)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.2 Search with observations\n",
    "\n",
    "지각 정의\n",
    "- 어떻게 환경에서 지각들이 생성되는지를 정의해야 함\n",
    "- 예를 들어 local vaccum world에서는 위치 센서, local dirt 센서가 있을 수 있다. \n",
    " - Percept(s) = [A, Dirty]\n",
    "- Fully observable은 모든 방에 대해서 global sensing이 가능한 상태\n",
    "- Sensorless 는 모든 state에 대해서 Percept(s) = null 인 것\n",
    "\n",
    "3-step\n",
    "- prediction : action에 대해서 예측되는 belief states\n",
    "- observing percept : local 지각 관측\n",
    "- update : 관측한 지각을 바탕으로 belief state 를 갱신\n",
    "\n",
    "update의 효과\n",
    "- 기존 belief state의 크기를 더 축소시킨다 ( 확실한 정보를 추가로 주는 것이므로 )\n",
    "- 각 지각들에 대해 disjoint한 partition을 가진다. \n",
    "\n",
    "![](./images/ch04/15.png)\n",
    "\n",
    "nondeterimism의 효과\n",
    "- prediction한 belief state 공간을 deterministic한 경우보다 더 확장해버리는 어려움이 생기지만\n",
    "- 반면에 확장을 통해서 더 확보 가능한 지각(Percept)의 종류가 더 다양해지는 효과 \n",
    "- 위의 그림 (b) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.3 Solving partially observable problems\n",
    "\n",
    "AND-OR tree를 구성해 가면서\n",
    "- Initial percept로부터 root belief state를 추정\n",
    "- OR node에서 action별로 branching\n",
    "- AND node에서 Percept(by local sensing)을 얻어서 branching 한다.\n",
    "\n",
    "아래 예에서는\n",
    "- initial percet = [A, Dirty] 로 부터 root states = [1,3]\n",
    "- OR node에서 Suck action 선택\n",
    "- local sensing 정보 [A, Clean]을 얻음 \n",
    "- belief state를 예측\n",
    "\n",
    "![](./images/ch04/16.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.4 An agent for partially observable environments\n",
    "\n",
    "예측-관측-갱신(prediction-observation-update) 반복\n",
    "\n",
    "예측에서는 belief state를 확장하고, 관측 후의 갱신에서는 belief state를 축소한다.\n",
    "\n",
    "이러한 것을 계속 반복한다.\n",
    "\n",
    "![](./images/ch04/17.png)\n",
    "\n",
    "![](./images/ch04/18.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Online search agents and unknown environments\n",
    "\n",
    "offline search\n",
    "- 실제 세계에 들어가기 전에 완벽히 solution을 계산한 후에 진입\n",
    "\n",
    "online search\n",
    "- action을 취하고 환경을 관측하고 이를 바탕으로 다시 action을 취하고,.. 계속 반복 \n",
    "- 완전히 환경에 대해 아무것도 모를 때, 어떤 상태들이 있는지, 어떤 action들이 가능한 것인지 전혀 모를때 반드시 필요한 탐색 전략\n",
    "\n",
    "\n",
    "### 4.5.1 Online search problems\n",
    "\n",
    "agent는 다음만 알고 있음\n",
    "- ACTIONS(s) : 특정 상태에서 가능한 action 목록 \n",
    "- step cost c(s,a,s') : action을 취해서 상태 천이가 일어난 후에 알수 있음\n",
    "- GOAL-TEST(s)\n",
    "\n",
    "agent는 RESULT(s,a)를 알 수 없음\n",
    "- 실제 action을 취해서 겪어봐야 알지, 해보기 전에는 다음 상태들에 대한 예측이 안됨\n",
    "- 아래 예에서 (1,1) 상태에서 ACTIONS(1,1)=[Up,Right] 만 알고\n",
    "- Result( (1,1), Up ) = (1,2) 인 것은 모른다!\n",
    "\n",
    "\n",
    "![](./images/ch04/19.png)\n",
    "\n",
    "\n",
    "Agent는 admissible heuristic function h(s)는 알 수 있음 \n",
    "- 특정 상태에서 goal 상태까지의 추정 거리 \n",
    "- ex) 미로 문제에서 goal 상태까지의 맨해튼 거리\n",
    "\n",
    "competitive ratio\n",
    "- search space를 완벽히 알고 있을 때의 solution path의 길이와\n",
    "- search space를 완벽히 모르고 탐험(exploration)을 해가며 찾은 solution path의 길이 사이의 비율\n",
    "- 최대한 작게 하는 것이 목표\n",
    "\n",
    "dead-end\n",
    "- competitive ratio가 무한대가 되는 상황\n",
    "- 한번 진입한 상태에서 이전상태로 돌아갈 방법이 없는 상황 (아래 그림의 a)\n",
    "- 어떠한 알고리즘도 dead end를 피할 수 있는 방안을 고안해 내지 못했다\n",
    "\n",
    "![](./images/ch04/20.png)\n",
    "\n",
    "Safely explorable\n",
    "- 단순히 우리가 푸는 문제가 dead-end 가 없는 안전하게 탐험할 수 있는 공간이라고 가정하자.\n",
    "\n",
    "no bounded competitive ratio\n",
    "- safelt explorable을 가정해도 bounded competitive ratio는 보장할 수 없다.\n",
    "- 위의 그림에서 (b) 처럼 path 자체가 unbounded되는 상황 \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
