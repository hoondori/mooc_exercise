{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference\n",
    "* [complete-tutorial-tree-based-modeling](https://www.analyticsvidhya.com/blog/2016/04/complete-tutorial-tree-based-modeling-scratch-in-python/)\n",
    "\n",
    "# 1. What is a Decision Tree ? How does it work ?\n",
    "\n",
    "* type of supervised learning algorithm  that is mostly used in classification problems. \n",
    "* works for both categorical and continuous input and output variables. \n",
    "\n",
    "![](./images/ch17_dt/1.png)\n",
    "\n",
    "\n",
    "decision tree identifies the most significant variable and it’s value that gives best homogeneous sets of population\n",
    "\n",
    "![](./images/ch17_dt/2.png)\n",
    "\n",
    "## Types of Decision Trees \n",
    "based on the type of target variable we have. It can be of two types\n",
    "* Categorical Variable Decision Tree : \n",
    "  * ex) YES or NO\n",
    "* Continuous Variable Decision Tree\n",
    "  * ex) predict customer income\n",
    "\n",
    "## Important Terminology related to Decision Trees\n",
    "![](./images/ch17_dt/3.png)\n",
    "\n",
    "\n",
    "## Advantages\n",
    "* **Easy to Understand** \n",
    "  * very easy to understand even for people from non-analytical background. \n",
    "  * It does not require any statistical knowledge to read and interpret them. \n",
    "  * Its graphical representation is very intuitive and users can easily relate their hypothesis.\n",
    "* Useful in Data exploration: \n",
    "  * fastest way to identify most significant variables and relation between two or more variables. \n",
    "  * we can create new variables / features that has better power to predict target variable.\n",
    "* Less data cleaning required: \n",
    "  * It is not influenced by outliers and missing values to a fair degree.\n",
    "* Data type is not a constraint\n",
    "  * It can handle both numerical and categorical variables.\n",
    "* Non Parametric Method: \n",
    "  * no assumptions about the space distribution and the classifier structure.\n",
    " \n",
    "\n",
    "## Disadvantages\n",
    "\n",
    "* **Over fitting**\n",
    "  * one of the most practical difficulty for decision tree models. \n",
    "  * This problem gets solved by setting constraints on model parameters and pruning \n",
    "* Not fit for continuous variables: \n",
    "  * While working with continuous numerical variables, decision tree looses information when it categorizes variables in different categories.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. How does a tree decide where to split?\n",
    "\n",
    "Degree of homogenity\n",
    "\n",
    "![](./images/ch17_dt/4.png)\n",
    "![](./images/ch17_dt/5.png)\n",
    "![](./images/ch17_dt/6.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. What are the key parameters of tree modeling and how can we avoid over-fitting in decision trees?\n",
    "\n",
    "## Setting constraints on tree size\n",
    "* greedy-search manner\n",
    "![](./images/ch17_dt/7.png)\n",
    "\n",
    "## Tree pruning\n",
    "\n",
    "optimal \n",
    "\n",
    "* We first make the decision tree to a large depth.\n",
    "* Then we start at the bottom and start removing leaves\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Are tree based models better than linear models?\n",
    "\n",
    "* Relationship between dependent & independent variable is well approximated by a linear model\n",
    "  * =>linear regression\n",
    "* high non-linearity & complex relationship between dependent & independent variables \n",
    "  * => a tree model\n",
    "* easy to explain to people\n",
    "  * => a decision tree model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What are ensemble methods in tree based modeling ?\n",
    "\n",
    "\n",
    "## Bagging\n",
    "Bagging is a technique used to reduce the variance of our predictions by combining the result of multiple classifiers modeled on different sub-samples of the same data set    \n",
    "\n",
    "![](./images/ch17_dt/8.png)\n",
    "\n",
    "\n",
    "* Create Multiple DataSets:\n",
    "  * Sampling is done with replacement\n",
    "* Build Multiple Classifiers:\n",
    "  * Classifiers are built on each data set.\n",
    "* Combine Classifiers:\n",
    "\n",
    "The combined values are generally more robust than a single model.\n",
    "\n",
    "It is called Random Forest\n",
    "![](./images/ch17_dt/9.png)\n",
    "\n",
    "## Boosting\n",
    "Boosting refers to a family of algorithms which converts weak learner to strong learners.\n",
    "\n",
    "Weak spam classifier :\n",
    "* Email has only one image file (promotional image), It’s a SPAM\n",
    "* Email has only link(s), It’s a SPAM\n",
    "* Email body consist of sentence like “You won a prize money of $ xxxxxx”, It’s a SPAM\n",
    "* Email from our official domain “Analyticsvidhya.com” , Not a SPAM\n",
    "* Email from known source, Not a SPAM    \n",
    "\n",
    "combine the prediction of each weak learner using methods like:\n",
    "* Using average/ weighted average\n",
    "* Considering prediction has higher vote\n",
    "\n",
    "XgBoost\n",
    "* https://xgboost.readthedocs.io/en/latest/\n",
    "\n",
    "### AdaBoost : Adaptive Boosting\n",
    "### Gradient boosting\n",
    "\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
