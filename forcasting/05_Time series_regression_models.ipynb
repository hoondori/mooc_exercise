{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 5 Time series regression models\n",
    "\n",
    "* forcast variable y가 predictor variable x에 선형적으로 관련성이 있다는 가정하에 모델링\n",
    "  * 예) 월별 판매량 y를, 광고 집행비 x로 선형적으로 설명\n",
    "* forcast variable\n",
    "  * regressand, dependent or explained \n",
    "* predictor variable\n",
    "  * regressors, independent or explanatory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Linear Regresssion\n",
    "\n",
    "* slope, intercept, error\n",
    "  * 에러는 실제 에러가 아니라, x로 설명되지 않는 factor들의 종합적 영향을 표현하는 것\n",
    "\n",
    "![](./images/ch05/01.png)\n",
    "![](./images/ch05/02.png)\n",
    "\n",
    "\n",
    "### Example: US consumption expenditure\n",
    "\n",
    "* 가처분 소득 변화율(x)와 지출 변화율(y)\n",
    "\n",
    "![](./images/ch05/03.png)\n",
    "![](./images/ch05/04.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Linear Regresssion\n",
    "\n",
    "![](./images/ch05/05.png)\n",
    "\n",
    "### Example: US consumption expenditure\n",
    "\n",
    "* 다양한 predictor 고려\n",
    "  * 산업 생산량 변화, 저축률 변화, 실업율 변화\n",
    "  \n",
    "![](./images/ch05/06.png)\n",
    "\n",
    "* scatterplot으로 개별 predictor와 forcasting variable사이의 관련성, predictor들끼리의 상관성 도식화\n",
    "  * 소득 증가와, 산업 생산량 증가와는 +의 상관관계\n",
    "  * 저축률 증가와, 실업률 증가와는 -의 상관관계\n",
    "![](./images/ch05/07.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## error term에 대한 가정\n",
    "\n",
    "* zero mean을 가져야 한다. 아니라면 systematic bias가 생김\n",
    "* autocorrelated되지 말아야 한다. 아니라면 forcasting이 비효율적이 된다. \n",
    "* predictor들과는 unrelated되지 말아야 한다. 아니라면 model에 반영되야할 어떤 시스템적인 특성이 더 있다는 말이다. \n",
    "* gaussian 분포를 갖되 constant varience까지 갖는다면 prediction interval 구할 때 매우 도움이 된다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Least squares estimation\n",
    "\n",
    "![](./images/ch05/08.png)\n",
    "\n",
    "![](./images/ch05/09.png)\n",
    "\n",
    "### Goodness-of-fit\n",
    "\n",
    "#### R-squared\n",
    "\n",
    "* square of the correlation between the observed yvalues and the predicted y_hat values.\n",
    "* reflects the proportion of variation in the forecast variable that is accounted for (or explained) by the regression model.\n",
    "  * 예를 들어 값이 0.74라면 74%의 variation을 설명하는 것이라고 볼 수 있다. \n",
    "* perfect fit이면 1, unrelated fit이라면 0\n",
    "\n",
    "![](./images/ch05/10.png)\n",
    "\n",
    "* 한계점 \n",
    "  * 부정확할 수 있다.\n",
    "    * predictor를 추가할 때마다 늘어만 간다. 절대 줄어들지 않는다. \n",
    "  * data마다 적절한 값 기준이 다르다. \n",
    "  * 이처럼 training data에 대한 R-squared값을 구하는 것보다는 test data에 대한 perf metric을 구하는 것이 훨씬 낫다. \n",
    "\n",
    "#### Standard error of regression\n",
    "\n",
    "* standard deviation of residuals\n",
    "* 에러의 누적 총량이 아니라 에러의 varience에 집중\n",
    "* k개의 predictor 개수로 보정\n",
    "\n",
    "![](./images/ch05/11.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Evaluating the regression model\n",
    "\n",
    "### residual\n",
    "* 관측값 - 예측값 \n",
    "* residual은 zero mean\n",
    "* redisual과 predictor와는 uncorrelated\n",
    "\n",
    "![](./images/ch05/12.png)\n",
    "![](./images/ch05/13.png)\n",
    "\n",
    "\n",
    "### ACF plot of residuals\n",
    "\n",
    "* 관측값은 autocorrelated가 강하다. 왜냐하면 previous(next) time 관측값은 current값과 연관이 있기 때문에\n",
    "* 마찬가지로 residual에서도 이러한 autocorrelation이 있는지를 조사하는 것이 중요 \n",
    "* 만약 autocorrelated되었다면\n",
    "  * 선형 모델링의 가정인 uncorrelated residual가정이 무너진다. \n",
    "  * 즉 시스템적으로 무언가 반영할 만한 정보가 있다는 뜻이고, 모델링이 덜 됬다는 신호\n",
    "  * 예측의 부정확성을 증가시키므로, prediction interval이 늘어나게 되는 결과 초래\n",
    "  \n",
    "![](./images/ch05/14.png)\n",
    "\n",
    "* Breusch-Godfrey test( or Lagrange Multiplier test) 를 통해 autocorrelated여부를 세밀하게 따지게 된다. \n",
    "  * 작은 p이면 uncorrelated\n",
    "\n",
    "![](./images/ch05/15.png)\n",
    "\n",
    "\n",
    "### Residual plots against predictors\n",
    "\n",
    "* 선형 모델에서는 predictor와 residual은 uncorrelated되어야 한다. \n",
    "* scatterplot을 통해서 이를 검증해 본다. \n",
    "* 만약 correlated되어 있다면, 더이상 선형 모델로는 부족하고, 비선형 모델링으로 전환해야 한다. \n",
    "\n",
    "![](./images/ch05/16.png)\n",
    "\n",
    "### Residual plots against fitted values\n",
    "\n",
    "* 어떤 패턴도 발견되서는 안된다. \n",
    "* 아니라면 소위, 에러 내에 이분산성(heteroscedasticity)이 있는 상황으로 의심됨 \n",
    "  * 에러의 varience가 x 값이 따라서 달라지는 현상\n",
    "\n",
    "![](./images/ch05/18.png)\n",
    "\n",
    "* 이분산성이란?\n",
    "  * variability of variable이 값에 따라서 달라지는 상황\n",
    "  * 아래 그림에서는 나이에 따른 소득 수준을 그린 것이다.\n",
    "  * 소득 편차는 나이대에 따라 다르다. 십대에서는 모두가 소득이 낮다. 하지만 20~30대 갈수록 빈부격차가 강화되고, 50~60대 가면 격차가 극에 달한다. \n",
    "  \n",
    "![](./images/ch05/17.png)\n",
    "\n",
    "\n",
    "### Outliers and influential observations\n",
    "\n",
    "* outlier란?\n",
    "  * extreme value \n",
    "* influential observation이란?\n",
    "  * 모델의 parameter값 결정에 많은 영향을 끼친 관측값들\n",
    "* outlier는 보통 influential하다고 말할 수 있다. \n",
    "\n",
    "![](./images/ch05/19.png)\n",
    "\n",
    "* outlier에 어떻게 대처할 것인가? \n",
    "  * 일단 어떻게 발견할 것인가? \n",
    "    * min/max와 같은 간단한 방법으로 수동 발견, scatterplot으로 눈대중으로, 좀더 정교한 통계 방법으로..\n",
    "  * unlikely outlier라고 판단되면 그냥 제거 \n",
    "    * 실제값이라기 보다는 잘못 기록된 값이라고 생각되면 제거 \n",
    "  * likely outlier라고 여거지면\n",
    "    * 제거한 경우의 fit과 제거하지 않은 경우의 fit 모두를 해보고 검토해 본다. \n",
    "    \n",
    "### Spurious regression\n",
    "\n",
    "* 시계열 데이터가 non-stationary한 경우도 있다. \n",
    "  * 즉 시간에 따라 분포가 달라진다 (change of mean, variance)\n",
    "  * 이런 경우 선형 모델에 미치는 영향은 어떻게 될까?\n",
    "* 비정적 시계열 데이터를 선형 모델링을 하면 Spurious regression이 된다. \n",
    "  * 과도한 R-sqaured와 autocorrelated residual이 관측\n",
    "  * 아래 예시에서는 서로 상관없는 기니의 쌀 생산량과 호주의 비행기 이용객 수를 억지로 선형 모델할 때 spurious regression이 생기는 경우 \n",
    "  \n",
    "![](./images/ch05/20.png)\n",
    "![](./images/ch05/21.png)\n",
    "![](./images/ch05/22.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Some useful predictors\n",
    "\n",
    "### Trend\n",
    "\n",
    "* x=t \n",
    "\n",
    "![](./images/ch05/23.png)\n",
    "\n",
    "### Dummy variable\n",
    "\n",
    "* predictor가 numeric이 아닌 categorical인 경우 \n",
    "* 예를 들어 Holiday 인지 아닌지, Yes or No\n",
    "* indicator variable로 변환하여 선형 모델에 반영\n",
    "\n",
    "### Seasonal dummy variables\n",
    "\n",
    "* 카테고리수가 3개 이상인 경우는 one-hot encoding을 한다. \n",
    "* all zero를 포함하므로 predictor의 개수는 카테고리수 - 1 \n",
    "* 해석\n",
    "  * measure of the effect of that category relative to the omitted category\n",
    "  * 요일 예시에서는 ommit된 일요일 대비한 효과\n",
    "\n",
    "\n",
    "![](./images/ch05/24.png)\n",
    "\n",
    "### Example: Australian quarterly beer production\n",
    "* 1개의 trend와 3개의 querterly variable (첫번째 쿼터 ommitted)\n",
    "\n",
    "![](./images/ch05/25.png)\n",
    "![](./images/ch05/27.png)\n",
    "![](./images/ch05/28.png)\n",
    "\n",
    "### Intervention variables\n",
    "\n",
    "* 일종의 외부 개입 효과를 나타내는 predictor\n",
    "  * ex) 경쟁사의 행위, 광고 효과, 산업계의 행동\n",
    "* 일회성이면 spike dummy variable로 묘사\n",
    "* 즉각적이며 영구적인 level-shift와 같은 효과가 되면 step variable로 묘사 \n",
    "* 영구적인 상승/하강 trend를 초래하면, piecewise linear로 묘사 \n",
    "\n",
    "\n",
    "### Distributed lags\n",
    "\n",
    "* 광고비와 같은 경우, 실제 효과는 늦게(lagged) 나타나는 predictor 이다. \n",
    "* 다음처럼 과거 집행한 광고비의 효과를 별도의 predictor로 묘사 \n",
    "\n",
    "![](./images/ch05/26.png)\n",
    "\n",
    "\n",
    "### Fourier series\n",
    "\n",
    "* 좀 더 긴 주기의 periodical한 시게열 데이터를 fit하는데 좋음 \n",
    "* quarterly data의 경우는 년간 4개의 data point밖에 없는 경우므로, seasonal dummy variable이 더 효과적\n",
    "* 반면, weekly data in a year의 경우는 m=52이므로, 푸리에로 fit하는게 더 적은 variable로 fit할 수 있음 \n",
    "\n",
    "![](./images/ch05/29.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 Selecting predictors\n",
    "\n",
    "* predictor들이 넘처나는데 어떻게 고를 것인가?\n",
    "* 권장하지 않는 접근법\n",
    "  * 하나의 predictor와 forcasting 변수 사이의 상관 관계를 단순 파악하고(ex. scatterplot), 그냥 버려버리는 경우\n",
    "  * fit을 돌려서 나온 p-value를 맹신해서 쉽게 버리는 경우 \n",
    "    * p-value는 misleading할 수 있다. \n",
    "* CV()함수를 사용해서 나온 metric들로 판단해보자 \n",
    "\n",
    "### Adjusted R-squared\n",
    "\n",
    "* R-squared 의 단점인 predictor추가시에 값이 무조건 증가하는 현상을 해소\n",
    "\n",
    "![](./images/ch05/30.png)\n",
    "\n",
    "\n",
    "### Cross-validation\n",
    "\n",
    "* leave-one-out cross-validation to selection predictors \n",
    "\n",
    "### Akaike’s Information Criterion\n",
    "\n",
    "* data fit와 동시에 model parameter 개수 최소화(small model 선호) \n",
    "\n",
    "![](./images/ch05/31.png)\n",
    "\n",
    "### Corrected Akaike’s Information Criterion\n",
    "\n",
    "* AIC는 large T일 경우에만 유효\n",
    "* small T인 경우는 predictor 개수 억제 효과가 사라짐\n",
    "* Correlated AIC는 larget T 제약을 완화시켜서, small T인 경우에도 동작\n",
    "  * https://www.researchgate.net/publication/242197194_171290_Model_Selection_Lecture_III_AIC_and_Corrected_AIC_AICc\n",
    "* large T라면, AIC와 비슷해진다. \n",
    "\n",
    "![](./images/ch05/32.png)\n",
    "\n",
    "\n",
    "### Schwarz’s Bayesian Information Criterion\n",
    "\n",
    "* AIC보다 predictor 개수 억제 효과가 더 있다. \n",
    "\n",
    "![](./images/ch05/33.png)\n",
    "\n",
    "### Which measure should we use?\n",
    "\n",
    "* 일반적으로 Adjusted R-squared를 많이 쓰지만. too many predictor가 선택되는 경향 있음 \n",
    "* 통계학자들은 대부분 BIC를 선호, true underlying model을 찾는 것이라고 생각하므로 \n",
    "* 하지만 현실적으로는 true model이 좋은 forcasting을 하는 것은 아니다. \n",
    "* 저자의 추천은 AIC, AICc, CV\n",
    "\n",
    "### 예시 : 미국 소비율 문제\n",
    "\n",
    "* predictor 후보 4개의 조합을 생각하면 총 16개의 모델 가능, 이를 위의 metric으로 측정해 보면 \n",
    "* 얼핏 보면 4개 다 쓴게 가장 좋아 보이나..\n",
    "  * 처음 네 개 row가 나머지보다 훨씬 낫다. => 즉 income하고, saving이 나머지보다 중요\n",
    "  * 처음 두 개 row가 거의 비슷하다. => 즉 production은 있으나마나\n",
    "  * 결론적으로는 production빼고 3개 정도 있으면 될 듯\n",
    "![](./images/ch05/34.png)\n",
    "\n",
    "\n",
    "### stepwise regression\n",
    "\n",
    "* 변수가 너무 많은 경우, 예를 들어 40개의 변수라면, 가능 모델은 2^40\n",
    "* backward-stepwise regression\n",
    "  * 모든 변수를 쓴 모델에서 출발해서, 하나를 뺄 때 개선 효과가 있으면 빼고 아니면 그대로, 이를 반복\n",
    "  * 이것도 변수가 매우 많으면 못할 짓 \n",
    "* forward-stepwise regression\n",
    "  * 1변수부터 시작해서 하나를 더할 때 개선 효과가 있으면 더하고 아니면 그대로, 이를 반복 \n",
    "* hybrid \n",
    "  * 적절 모델에서 출발해서, forward/backward를 혼합해서 \n",
    "* 위 모든 방법은 best model로 귀결을 보장하지는 않지만 적절한 개선 모델로는 나아간다. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.6 Forecasting with regression\n",
    "\n",
    "### Ex-ante versus ex-post forecasts\n",
    "\n",
    "* Ex-ante forcast\n",
    "  * before the fact\n",
    "  * 해당 시점까지 주어진 fact만을 가지고 forcast를 하는 것 \n",
    "  * forcast of predictor가 필요 \n",
    "    * ex) 미국 소비율 forcast의 경우, y_t를 예측하려면 x_t가 있어야 하나, x_t는 현재 없는 상황\n",
    "      * 이 경우 x_t-1로 x_t를 추정한 후, 이를 바탕으로 y_t를 추정\n",
    "  * 진짜 예측이라고 볼 수 있다.\n",
    "* Ex-post forcast\n",
    "  * after the fact\n",
    "  * 미래의 fact도 동원해서라도 forcast를 하는 것\n",
    "  * 실제 예측이라기 보다는 forcasting model의 연구를 위한 방법\n",
    "  \n",
    "#### Example: Australian quarterly beer production\n",
    "\n",
    "* 호주 맥주량 예측의 경우, predictor가 deterministic한 것이라서(즉 forcast of predictor가 가능해서) \n",
    "  * Ex-ante와 Ex-post가 같은 경우이다. \n",
    "\n",
    "![](./images/ch05/35.png)\n",
    "![](./images/ch05/36.png)\n",
    "\n",
    "### Building a predictive regression model\n",
    "\n",
    "* 아예 미래를 예측하는 선형 모델을 만들어 버리면, ex-ante forcast에 대한 고민이 없지 않을까?\n",
    "* 광고비 집행 예시처럼 효과가 늦게 나타나는 경우도 더 잘 설명 가능할 것이다. \n",
    "\n",
    "![](./images/ch05/37.png)\n",
    "\n",
    "### Prediction Interval\n",
    "\n",
    "* 다음과 같은 간단한 regression case에 대해서 PI 구하기 \n",
    "\n",
    "![](./images/ch05/38.png)\n",
    "\n",
    "* Std error of regression\n",
    "![](./images/ch05/39.png)\n",
    "\n",
    "* PI\n",
    "  * mean과의 차이가 크면 예측 불확실성이 커진다. \n",
    "  * std error 가 작으면 예측 불확실성이 커진다.\n",
    "![](./images/ch05/40.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.7 Matrix formulation\n",
    "\n",
    "일반적인 regression에 대해서 PI를 구하는 수학적 방법\n",
    "\n",
    "* Multiple linear regression\n",
    "\n",
    "![](./images/ch05/41.png)\n",
    "\n",
    "* vectorized-notation of MLR\n",
    "\n",
    "![](./images/ch05/42.png)\n",
    "\n",
    "* Least mean square minimization objective\n",
    "\n",
    "![](./images/ch05/43.png)\n",
    "\n",
    "* Solution of LMS minimization\n",
    "\n",
    "![](./images/ch05/44.png)\n",
    "\n",
    "* redisual varience\n",
    "![](./images/ch05/45.png)\n",
    "\n",
    "* prediction for future predictors x_star\n",
    "\n",
    "![](./images/ch05/46.png)\n",
    "\n",
    "* PI\n",
    "  * 반영된 것 \n",
    "    * error due to systematic error : 즉 미반영된 외부 factor들의 불확실함으로 인한 오류 반영\n",
    "    * error due to estimation of model\n",
    "  * 미반영 \n",
    "    * error due to new datapoint x_star\n",
    "  * 미반영된 것들을 고려하면 PI는 과소 계산되었다고 볼 수 있다.\n",
    "![](./images/ch05/47.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.8 Nonlinear regression\n",
    "\n",
    "* 비선형 모델이 필요할 때가 많다. \n",
    "* 간단하게는 X,Y에 대해 먼저 transformation을 하고, 이들 간의 선형적인 모델링을 하는 것 \n",
    "  * 식 자체는 비선형이지만, 모델 파라미터 입장에서는 여전히 선형이다. \n",
    "\n",
    "### Log-Log Transform\n",
    "\n",
    "* Log transformation을 X,Y에 대해서 수행 \n",
    "* 0보다 커야 한다.\n",
    "* 0을 포함하는 양수 X에 대해서는 log(x+1) 처럼 변형해서 사용\n",
    "* Log-Linear, Linear-Log 도 있다. \n",
    "![](./images/ch05/48.png)\n",
    "\n",
    "\n",
    "### Piecewise-linear\n",
    "\n",
    "* 구간별로 기울기가 다른 선형 \n",
    "* 다음은 2개의 piece로 된 X domain predictor variable 형성\n",
    "  * 1개의 knob\n",
    "![](./images/ch05/49.png)\n",
    "\n",
    "\n",
    "### Regression Spline\n",
    "\n",
    "* Picewise-linear의 일반화 \n",
    "* hyperparameter는 몇 개의 knob을 쓸 것인가랑, knob들의 위치 정하기 \n",
    "  * 자동으로 해주는 알고리즘이 있다고 함 \n",
    "![](./images/ch05/50.png)\n",
    "\n",
    "### Piecewise cubics and Cublic regression spline\n",
    "\n",
    "* more smooth\n",
    "* continuous joined up\n",
    "\n",
    "![](./images/ch05/51.png)\n",
    "\n",
    "\n",
    "### Forecasting with a nonlinear trend\n",
    "\n",
    "* trend가 있는 경우, 선형적으로 기울기가 있는 데이터 \n",
    "* cubic으로 모델링을 하는 경우, unrealistic forcast가 나올 수도 있으니 비추천 \n",
    "* trend가 구간에 따라 달라진다면(구부러진다면) piecewise linear로 모델링 \n",
    "  * beta_1은 첫번째 구간의 기울기, beta_1+beta_2는 두번째 구간의 기울기 \n",
    "![](./images/ch05/52.png)\n",
    "\n",
    "\n",
    "### Example: Boston marathon winning times\n",
    "\n",
    "* 관찰\n",
    "  * 마라톤 완주 시간이 전반적으로는 계속 줄어들어 왔다. => downward trend 존재 \n",
    "  * 하지만 nonlinear한 패턴이 있는 듯 하다. \n",
    "  * heteroscedasticity도 있는 듯 하다 (decreasing variation over time)\n",
    "    * ~1940년대까지는 변동성이 심함 \n",
    "    * 1940~1980년대는 꾸준히 감소 \n",
    "    * 1980~는 almost flat\n",
    "![](./images/ch05/54.png)\n",
    "* 다양한 방법으로 fit\n",
    "  * exponential trend = log-linear \n",
    "    * exponential trend가 linear trend보다는 heteroscedasticity를 보다 잘 표현\n",
    "  * 1940과 1980을 knob을 사용해서 piecewise linear 모델링 \n",
    "  * cubic은 best fitting이기는 하지만 unrealistic forcast이다 (앞으로도 flat이 뻔히 예상되는데도..감소를 예측하고 있다.)\n",
    "  \n",
    "  \n",
    "![](./images/ch05/53.png)\n",
    "\n",
    "\n",
    "* natural cubic smoothing splines\n",
    "  * impose some constraints ?\n",
    "![](./images/ch05/55.png)\n",
    "![](./images/ch05/56.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.9 Correlation, causation and forecasting\n",
    "\n",
    "### Correlation is not causation\n",
    "\n",
    "* X가 Y를 Cause한 것일 수도 있고, Y가 X를 Cause한 것일 수도 있다.\n",
    "* 또다른 변수 사이의 개입이 있을 수 있다. \n",
    "  * 아이스크림이 판매 개수가 높다, 해변의 익사자가 늘어닌다.   \n",
    "  * 실은\n",
    "    * 여름날씨가 덥다 => 아이스크림 판매가 높다 \n",
    "    * 여름날씨가 덥다 => 해변 익사자가 늘어닌다. \n",
    "* 결론\n",
    "  * correlation은 분명 forcast에 도움을 주지만, (심지어 전혀 인과관계가 없더라도) 보다 좋은 모델은 인과관계를 분석한 변수들을 사용하는 것이 낫다.\n",
    "\n",
    "### Confounded predictors\n",
    "\n",
    "* predictor 사이에 연관관계가 있는 경우 \n",
    "\n",
    "### Multicollinearity and forecasting\n",
    "\n",
    "* 두 개의 predictor가 거의 동일한 정보를 제공할 때, 양자는 muliticollinear하다.\n",
    "* correlation coefficient가 +1이나 -1에 근접할 정도로 아주 연관될 때 \n",
    "* ex) Foot Size, Height\n",
    "* ex) dummy variable trap\n",
    "* Multicollinearity는 estimation of coefficient를 unreliable하게 한다. \n",
    "  * uncertainty of estimation 증가\n",
    "  * statistical test (Ex. t-test) on regression unreliable\n",
    "  * 각 predictor의 forcast에 대한 기여도 측정도 불안정해짐 \n",
    "  * predictor가 historical range에서 벗어나면 특히 forcast가 unreliable해짐 \n",
    "* 걱정할 것은 없다. \n",
    "  * 좋은 통계 프로그램이라면(이 효과를 억제하는 알고리즘 반영)\n",
    "  * 기여도 측정에 관심없다면 \n",
    "  * predictor가 historical range 내에 있다면 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
