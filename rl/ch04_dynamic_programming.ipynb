{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ch 4. Dynamic Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DP의 정의 및 한계\n",
    "- 환경이 완전한 Markov decision process(MDP)로 주어질 때, 최적화된 정책을 계산하는 일련의 알고리즘 집합\n",
    "- 완전환 MDP, 많은 계산 요구량으로 인해 한계는 있다.\n",
    "- 하지만 다른 모든 알고리즘의 기본 바탕을 제공해주므로 중요하다.\n",
    "\n",
    "finite 가정\n",
    "- finite/discrete MDP\n",
    " - finite state, finite action\n",
    " - 정확한(exact) 해를 찾을 수 있다.\n",
    "- infinite/continous 에도 적용할 수 있으나 근사해(approximate)를 찾는 것에 그침\n",
    "\n",
    "DP의 핵심\n",
    "- 최적화된 정책을 찾기 위해 value function을 이용\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review on Ch3\n",
    "\n",
    "### 특정 정책에 대한 state-value function\n",
    "![](./images/ch04/01.png)\n",
    "![](./images/ch04/02.png)\n",
    "\n",
    "### 특정 정책에 대한 action-value function\n",
    "![](./images/ch04/03.png)\n",
    "\n",
    "### 특정 정책에 대한 backup diagram\n",
    "![](./images/ch04/04.png)\n",
    "\n",
    "### 최적 정책에 대한 최적 state-value function\n",
    "![](./images/ch04/05.png)\n",
    "![](./images/ch04/06.png)\n",
    "\n",
    "### 최적 정책에 대한 최적 action-value function\n",
    "![](./images/ch04/07.png)\n",
    "\n",
    "### 최적 정책에 대한 backup diagram\n",
    "![](./images/ch04/08.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 정책 평가(Policy evaluation)\n",
    "\n",
    "수립된 정책으로 인해서 세상에 영향을 끼치는 것, 여기서는 value function에 영향을 주는 것\n",
    "\n",
    "\n",
    "### stochastic 정책이 고정되어서 주어졌을 때, value function\n",
    "- 존재하고(혹은 수렴하고), 유일하게 존재한다. \n",
    "- N개의 unknown, N개의 방정식 => 확실한(exact) 해를 구할 수 있다. 다만 매우 지루하다.\n",
    "\n",
    "![](./images/ch04/09.png)\n",
    "\n",
    "\n",
    "### Iterative한 방식으로 구해보자 \n",
    "- update rule은 사실 bellman 방정식과 다를 바 없다.\n",
    "- 수렴할까? 수렴한다. \n",
    "\n",
    "![](./images/ch04/10.png)\n",
    "\n",
    "### Iterative policy evaluation의 구현\n",
    "- 각 상태의 value를 업데이트하기 위해서는 이전 상태들의 value를 저장해 놓고 있어야 한다.\n",
    "- 즉 두 개의 array, old array, new array를 가지고 있다가 old를 기반으로 new를 갱신한다.\n",
    "- 상태수가 너무 많으면 이것도 부담된다. \n",
    "- 대안은? array 하나만 유지하고 in-place로 그냥 갱신하자 \n",
    "- 이래도 되나? 수학적 엄밀성으로는 안되겠지만, 공학적으로는 가능. \n",
    "  - 이래도 수렴한다. 그것도 더 빨리. \n",
    "- 언제까지 하나? 갱신된 value의 최대값조차도 미미하게 갱신될때까지  \n",
    "\n",
    "![](./images/ch04/11.png)\n",
    "\n",
    "### WildML 구현 \n",
    "- [https://github.com/dennybritz/reinforcement-learning/blob/master/DP/Policy%20Evaluation%20Solution.ipynb](https://github.com/dennybritz/reinforcement-learning/blob/master/DP/Policy%20Evaluation%20Solution.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4  x 4 gridworld 예제\n",
    "\n",
    "![](./images/ch04/12.png)\n",
    "![](./images/ch04/13.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 정책 개선(policy improvement)\n",
    "\n",
    "세상의 의견을 들어 기존 정책보다 나은 정책을 입안하는 것\n",
    "\n",
    "### 항상 더 좋은 정책은 있는 법이다. \n",
    "- 기존 정책이 최적 정책이 아닌 한은 모든 상황(상태)에서 기존의 정책보다 나은 정책을 항상 찾을 수 있다. \n",
    "- 적어도 하나의 상태 이상에서 기존보다는 향상된 정책이어야 한다.\n",
    "  - there must be strict inequality at at least one state\n",
    "  \n",
    "![](./images/ch04/14.png)\n",
    "![](./images/ch04/15.png)\n",
    "\n",
    "\n",
    "### 이렇게 개선된 정책은 사실은 greedy하다. ( new greedy policy)\n",
    "- 먼 미래까지 고려한 게 아니라 달랑 one-step ahead만 고려한 개선에 불과 \n",
    "\n",
    "![](./images/ch04/16.png)\n",
    "\n",
    "### 더 이상 개선할 것이 없게 되면 최적 정책이 된다.\n",
    "- 이 때 정책 개선은 bellman equation for optimal policy 로 동일하게 유도된다.\n",
    "\n",
    "![](./images/ch04/17.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 정책 선순환(Policy Iteration)\n",
    "\n",
    "정책 입안 => 세상 적용 후 피드백 => 정책 수정 => 세상 적용 후 피드백 => 반복....\n",
    "\n",
    "![](./images/ch04/18.png)\n",
    "\n",
    "### 장점과 단점\n",
    "- 놀랍게도 매우 빠르게 수렴된다. \n",
    "- 다만 광대한 상태 공간을 매우 많이 반복적으로 훑어야(sweep)한다. \n",
    "\n",
    "![](./images/ch04/19.png)\n",
    "\n",
    "### WildML 구현 \n",
    "- [https://github.com/dennybritz/reinforcement-learning/blob/master/DP/Policy%20Iteration%20Solution.ipynb](https://github.com/dennybritz/reinforcement-learning/blob/master/DP/Policy%20Iteration%20Solution.ipynb)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Value iteration \n",
    "\n",
    "정책 회전의 단점은 policy evaluation시에 드넓은 상태 공간을 반복적으로 훑어야(sweep)한다는 점\n",
    "\n",
    "\n",
    "### Truncate policy evaluation\n",
    "- 각 상태 공간에 대한 iteration시에 매우 정확히 수렴할 때까지 기다리지 말고 그냥 적당히 수렴하고 빠져나오자\n",
    "\n",
    "### value iteration in single sweep\n",
    "- policy evaluation 단계에서 상태 공간을 딱 한번만 훑기( just one sweep )\n",
    "- policy evaluation과 policy improvment를 한번에\n",
    "- 알고보니 bellman optimality equation과 동일\n",
    "\n",
    "![](./images/ch04/20.png)\n",
    "\n",
    "### 구현\n",
    "- policy evaluation 구현과 max 빼고는 다 똑같음\n",
    "\n",
    "![](./images/ch04/21.png)\n",
    "\n",
    "### WildML 구현\n",
    "- [https://github.com/dennybritz/reinforcement-learning/blob/master/DP/Value%20Iteration%20Solution.ipynb](https://github.com/dennybritz/reinforcement-learning/blob/master/DP/Value%20Iteration%20Solution.ipynb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 예제 : Gambler Problem\n",
    "\n",
    "- 동전 던질 때마다 돈 걸고 이기면 건 만큼 따기\n",
    "- 자본이 100$ 될때까지 하고 다 잃으면 episode 종료\n",
    "- 상태 S = {1,2, 99}, Action = { 0, 1, min(s, 100-s)}\n",
    "- value iteration으로 학습\n",
    "\n",
    "기괴한 최적 정책이 나온 이유는?\n",
    "- 수중에 50있을 때 전부 다 거는 게 최적\n",
    "- 수중에 51이 있을 때는 1달러만 거는 게 최적\n",
    "- 목표가 100달러를 넘는 게 목표가 아니라 정확히 100달러가 되게 하는 것 때문인 듯\n",
    "  - 즉 51달러 일 때는 차라리 1달러 잃고 50달러로 맞춘 후 100달러가 되게\n",
    "  \n",
    "![](./images/ch04/22.png)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Asynchronous Dynamic Programming\n",
    "\n",
    "상태 공간이 너무 넓으면 단 한번의 sweep도 상당히 부담스럽다. \n",
    "\n",
    "모든 상태를 빠짐없이, 체계적으로(systematic)하게 처리하던 기존 방식을 완화하는(relax) 대안을 찾아보자\n",
    "\n",
    "### in-place 방식 \n",
    "- 공간적으로 단 하나의 array만 유지하면 되서 효과적\n",
    "- 개선된 값을 더 빠르게 이용할 수 있어서 효과적\n",
    "\n",
    "### 우선순위를 가진 훑기 (prioritized sweep)\n",
    "- 순서대로 sweep하기 보다는 중요한 state 먼저\n",
    "- 개선 효과가 큰 state부터\n",
    "\n",
    "### real-time DP\n",
    "- 모든 state가 아니라 실제 agent가 방문한 곳, 혹은 그 근처 state만 고려\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6 Generalized Policy Iteration\n",
    "\n",
    "value function과 정책이 상호 작용해서 서로 발전하는 모델을 일반화 한 것 \n",
    "\n",
    "경쟁(competing)하면서도 협력(cooperate)하면서 최적의 상태로 나아간다.\n",
    "\n",
    "\n",
    "<img style=\"float: left;\" src=\"./images/ch04/23.png\" width=\"200\">\n",
    "<img style=\"float: center;\" src=\"./images/ch04/24.png\" width=\"400\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.7 Efficienct of Dynamic programming\n",
    "\n",
    "매우 큰 문제에는 적합하지 않은 것이 사실이다.\n",
    "\n",
    "### 계산 복잡성은 그럼에도 불구하고 polynomial 정도 수준에 그친다.\n",
    "- 상태 n, action k라고 할 때 시간 복잡도는 n과 k로 구성된 polynomial function정도 수준 \n",
    "- 다른 경쟁 알고리즘인 direct search나 linear programming보다 낫다. \n",
    "\n",
    "### curse of dimensionality에 취약하기는 하나\n",
    "- 상태 s가 고차원으로 표현될 때\n",
    "- 이 저주는 해당 문제의 어려움일 뿐이지 DP의 약점은 아니다. \n",
    "\n",
    "### synchronous DP보다는 Asynchronous DP를 사용하면 시간/공간 복잡도를 낮출 가능성이 있다. \n",
    "- 모든 상태에 대한 계산과 값 유지를 할 필요가 적어지므로\n",
    "- optimal policy로의 trajectory에는 상대적으로 적은 수의 상태들만 관여하므로\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
