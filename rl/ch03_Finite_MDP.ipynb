{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ch 3. Finite Markov Decision Processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Agent-Environment Interface\n",
    "\n",
    "3대 구성요소\n",
    "- Agent \n",
    " - learner or decision maker\n",
    "- Environment\n",
    " - agent와 상호 작용, agent 밖의 모든 것\n",
    "- reward\n",
    " - agent가 행동을 하고, 이것이 환경에 영향을 끼치고, 환경이 이에 대한 응답으로 주는 값\n",
    "\n",
    "![](./images/ch03/01.png)\n",
    "\n",
    "정책\n",
    "- 특정 상태에서 특정 행동을 결정하는 것.\n",
    "- deterministic or stochastic\n",
    "- RL이란 agent가 경험을 통해서 어떻게 자신의 정책을 효과적으로 개선시켜 나갈 것인지에 대한 것이다. \n",
    "\n",
    "Agent와 환경 사이의 모호함\n",
    "- 일반적 룰은 agent가 임의로 control할 수 없는 것은 환경이다. \n",
    "- reward가 어떻게 결정되는지 agent에게 일부 알려질 수는 있지만 이를 생성하는 주체는 엄연히 환경이다. \n",
    "- 아는 것과 제어하는 것의 차이(단순히 아는 것이 아니라 제어해야 agent의 일부가 된다)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Goals and Rewards\n",
    "\n",
    "### Agent goal\n",
    "- 즉각적 보상을 최대화하는 것이 아니라 장기적으로 축적된 보상(cumulative reward in the long run)을 최대화 하는 것 \n",
    "- reward를 우리가 성취하기 위한 signal을 의미하도록 설정하는 것이 중요하다. \n",
    " - 우리가 성취하는 것을 어떻게 얻는지에 대한 signal이 아니라 우리가 얻고자 하는 그 자체에 대한 signal이어야 한다.\n",
    " - The reward signal is your way of communicating to the robot what you want it to achieve, not how you want it achieved\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Returns\n",
    "\n",
    "### maximize expected return\n",
    "- 하나의 episide에서 발생한 모든 reward의 합을 최대화\n",
    "- t=T에서는 terminal state에 도달한 것\n",
    "- continous task에서는 T가 무한대이다. \n",
    "\n",
    "![](./images/ch03/02.png)\n",
    "\n",
    "### maximize expected discounted return\n",
    "- future reward의 현재 가치를 결정\n",
    "- discount rate가 0인 경우 근시안적(mypoic)\n",
    "- discount rate가 1에 근접하는 경우 farsighted\n",
    "\n",
    "![](./images/ch03/03.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Markov Property\n",
    "\n",
    "### State signal\n",
    "- agent에게 전달해 주는 정보 그 자체이다.\n",
    "- 이를 기반으로 agent는 어떤 액션을 취할지를 결정한다.\n",
    "- state signal은 측정값과 같은 immediate sensations 뿐만 아니라 그 이상의 highly processed된 정보도 포괄한다.\n",
    "- state는 immediate sensation 과 그 이전 상태 혹은 과거 sensation과의 융합을 통해서 구성된다. \n",
    "- 환경에 대한 모든 정보를 agent에게 알려주는 것을 의미하는 것은 아니다. \n",
    " - agent한테 알려줄 수 없는 환경의 은닉된 정보도 많다. \n",
    "\n",
    "### Markov property of state signal\n",
    "- state signal에는 과거의 모든 sensation을 요약/함축한 정보가 담겨 있다.\n",
    "- 요약 정보이지 complete history가 아니다. \n",
    "- 아래처럼 과거 모든 정보에 dependent하지 않고 오직 이전 정보에만 dependent하다. \n",
    " - independence of path property\n",
    "- one-step dynamics\n",
    " \n",
    "![](./images/ch03/04.png)\n",
    "![](./images/ch03/05.png)\n",
    "\n",
    "### Aproximation to Markov propery\n",
    "- 비록 state signal이 non-markov이어도, 여전히 markov propery를 만족하는 것으로 가정하고 문제를 풀어도 효과적이다. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 Markov Decision Process\n",
    "\n",
    "### finite MDP\n",
    "- state와 action 공간이 finite하고 markov property를 만족하는 경우\n",
    "- 현대 RL 문제의 90% 이상이 이에 해당\n",
    "\n",
    "![](./images/ch03/06.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7 Value functions\n",
    "\n",
    "### 의미\n",
    "- 해당 state에 Agent가 있는 것이 얼마나 좋은 것인지에 대한 수치화\n",
    "- 얼마나 좋은지는 미래의 보상이 얼마나 클 것인지에 대한 것\n",
    "\n",
    "\n",
    "### state value function for the policy\n",
    "\n",
    "![](./images/ch03/07.png)\n",
    "\n",
    "### action value function for the policy\n",
    "\n",
    "![](./images/ch03/08.png)\n",
    "\n",
    "### state value function calcuation\n",
    "\n",
    "![](./images/ch03/09.png)\n",
    "\n",
    "### backup-diagrm\n",
    "\n",
    "![](./images/ch03/10.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.8 Optimal value function\n",
    "\n",
    "### optimal policy\n",
    "![](./images/ch03/11.png)\n",
    "\n",
    "### optimal state value function\n",
    "![](./images/ch03/12.png)\n",
    "\n",
    "### optimal action value function\n",
    "![](./images/ch03/13.png)\n",
    "\n",
    "### Bellman optimality equation\n",
    "![](./images/ch03/14.png)\n",
    "![](./images/ch03/15.png)\n",
    "\n",
    "### backup-diagram for Bellman optimality equation\n",
    "![](./images/ch03/16.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
