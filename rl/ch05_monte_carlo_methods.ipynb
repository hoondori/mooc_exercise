{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ch 5. Monte Carlo Methods\n",
    "\n",
    "## 환경에 대한 지식은 없다. 실제 경험을 통해 배운다.\n",
    "- p(s|a), p(s',r|s,a) 등의 환경의 지식이 없다.\n",
    "- simulation된 에피소드로부터 배운다.\n",
    "- by averaing sample returns\n",
    "- 에피소드 하나씩 배워 나가는 incremental learing 방식이다.\n",
    "- 정책 선순환(GPI: general policy iteration)와 같은 개념이 동일하게 적용되서 최적화(optimality)로 이끈다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Monte Carlo Prediction ( sample 기반 정책 평가 )\n",
    "\n",
    "### state value를 averaging sample return으로 배운다.\n",
    "- 원래 state value는 expected return임\n",
    "- law of large number 에 의해서 sample이 많으면 average -> expectation으로 된다. \n",
    "- episode 내에 등장하는 state에 대한 return을 계속 누적 평균하면 expected return 으로 빠르게 수렴한다.\n",
    " - first-visit MC는 episode에서 특정 state가 첫번째 등장했을때만 고려한 것\n",
    " - every-visit MC는 episode에서 특정 state가 등장할 때마다 고려한 것\n",
    "\n",
    "![](./images/ch05/01.png)\n",
    "\n",
    "### 환경 지식이 있을때라도 MC는 유용하다.\n",
    "- 환경 지식이 있다하더라도 DP에 필요한 확률들, p(s'r|s,a)와 같은 것을 쉽게 추정하기 어려움\n",
    "- 반면 episide를 generation하는 것이 쉬운 경우라면, 예를 들어 black-jack, episode기반 MC를 하는 것이 더 잇점이 있다.\n",
    "\n",
    "### Backup diagram은 episode 그 자체이다.\n",
    "- 하나의 episode내의 모든 state sequence 자체가 back-up diagram이 된다.\n",
    "- 즉 각각의 state의 update는 다른 state에 대해 독립적(independent)이다.\n",
    " - 나쁜 점은 DP처럼 boost 효과가 없다는 것\n",
    " - 좋은 점은 state의 갱신이 다른 state와 무관하므로 계산 시간에 잇점\n",
    " - 또다른 좋은 점은 오직 관심있는 영역의 상태 공간에 대한 episode들을 생성하고, 이들을 전체와 무관하게 학습할 수 있다는 점\n",
    "\n",
    "![](./images/ch05/02.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Monte Carlo Estimation of Action Values\n",
    "\n",
    "### action-value까지 추정해야 정책 개선까지 나아갈 수 있다.\n",
    "- q(s,a) given policy\n",
    "- state-action 쌍으로 동일한 원리로 MC를 적용해서 action-value를 추정한다.\n",
    "\n",
    "### 가장 큰 단점은 never-visit 문제이다.\n",
    "- state-value 추정과는 다르게 state-action 쌍은 sample episode를 많이 발생시켜도 관측이 전혀 안되는 경우가 허다하다\n",
    "- 해당 state에서의 모든 action들에 대한 추정을 해줘야 정책 개선을 하는데 용이한데 이렇게 비관측 상황이 많아지는 것은 심각한 문제\n",
    "\n",
    "### exploraing start로 never-visit 문제에 대처하자\n",
    "- episode의 시작을 non-zero의 확률로 특정 state-action pair에서 시작시킨다. \n",
    "- 이러면 수많은 시뮬레이션을 진행하다보면 모든 action에 대해 관측열이 발생할 것이다.\n",
    "\n",
    "### non-zero 확률 기반 stochastic 정책이 더 자연스럽다\n",
    "- explorating start는 단순히 starting poinit만 임의 설정하는 것이므로 실제의 interation with environment와는 동떨어진 조치\n",
    "- stochastic 정책을 세우고, 모든 action에 대해서 non-zero 확률을 설정하는 것으로 대안\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## 5.3 Monte Carlo Control"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
